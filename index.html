<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AI Learning â€” Ram's Journey</title>
    <style>
        * { margin: 0; padding: 0; box-sizing: border-box; }

        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
            background: #191919;
            color: #e0e0e0;
            height: 100vh;
            overflow: hidden;
        }

        /* Layout */
        .layout {
            display: flex;
            height: 100vh;
        }

        /* Sidebar */
        .sidebar {
            width: 260px;
            min-width: 260px;
            background: #202020;
            border-right: 1px solid #2d2d2d;
            padding: 20px 12px;
            overflow-y: auto;
            display: flex;
            flex-direction: column;
        }
        .sidebar-brand {
            padding: 8px 12px 20px;
            font-size: 14px;
            font-weight: 700;
            color: #9ca3af;
            letter-spacing: -0.3px;
        }
        .sidebar-brand span { color: #e0e0e0; }
        .sidebar-link {
            display: block;
            padding: 6px 12px;
            margin: 1px 0;
            color: #9ca3af;
            text-decoration: none;
            font-size: 14px;
            border-radius: 6px;
            transition: all 0.15s;
            line-height: 1.5;
        }
        .sidebar-link:hover {
            background: #2d2d2d;
            color: #e0e0e0;
        }
        .sidebar-link.active {
            background: #2d2d2d;
            color: #ffffff;
        }
        .sidebar-section-label {
            padding: 12px 12px 4px;
            font-size: 11px;
            font-weight: 700;
            color: #525252;
            letter-spacing: 1px;
        }
        .sidebar-divider {
            height: 1px;
            background: #2d2d2d;
            margin: 8px 12px;
        }
        .sidebar-footer {
            margin-top: auto;
            padding: 16px 12px 8px;
            font-size: 11px;
            color: #525252;
        }

        /* Content */
        .content {
            flex: 1;
            overflow-y: auto;
            padding: 40px 60px;
        }

        /* Pages (sections) */
        .page {
            display: none;
            max-width: 720px;
            animation: fadeIn 0.2s;
        }
        .page.active { display: block; }
        @keyframes fadeIn { from { opacity: 0; } to { opacity: 1; } }

        .back-link {
            display: inline-block;
            font-size: 13px;
            color: #8b5cf6;
            text-decoration: none;
            margin-bottom: 16px;
            padding: 4px 0;
            transition: color 0.15s;
        }
        .back-link:hover {
            color: #a78bfa;
        }

        .page-breadcrumb {
            font-size: 12px;
            color: #525252;
            margin-bottom: 8px;
            letter-spacing: 0.5px;
        }
        .page-title {
            font-size: 32px;
            font-weight: 700;
            color: #ffffff;
            margin-bottom: 6px;
            letter-spacing: -0.5px;
        }
        .page-meta {
            font-size: 14px;
            color: #6b7280;
            margin-bottom: 36px;
        }

        /* Content Blocks */
        .content-block {
            margin-bottom: 32px;
        }
        .block-label {
            font-size: 12px;
            font-weight: 700;
            letter-spacing: 0.5px;
            color: #6b7280;
            margin-bottom: 16px;
            padding-bottom: 8px;
            border-bottom: 1px solid #2d2d2d;
        }

        /* Kid Story */
        .kid-block {
            padding: 24px;
            background: #1c1917;
            border-radius: 8px;
            border-left: 3px solid #f59e0b;
        }
        .story-para {
            font-size: 15px;
            line-height: 1.85;
            color: #fef3c7;
            margin-bottom: 14px;
            font-family: Georgia, 'Times New Roman', serif;
        }
        .story-para:last-child { margin-bottom: 0; }

        /* Tech Story */
        .tech-block {
            padding: 24px;
            background: #1a1a2e;
            border-radius: 8px;
            border-left: 3px solid #8b5cf6;
        }
        .tech-para {
            font-size: 14px;
            line-height: 1.75;
            color: #c4b5fd;
            margin-bottom: 12px;
        }

        /* Share Story Button */
        .share-story-btn {
            display: inline-block;
            margin-top: 24px;
            padding: 10px 20px;
            background: #2d1b69;
            color: #c4b5fd;
            border: 1px solid #8b5cf6;
            border-radius: 8px;
            font-size: 14px;
            font-weight: 600;
            text-decoration: none;
            cursor: pointer;
            transition: all 0.15s;
        }
        .share-story-btn:hover {
            background: #3b2480;
            color: #e0d4ff;
            box-shadow: 0 2px 12px rgba(139, 92, 246, 0.25);
        }

        /* Diagram */
        .diagram-box {
            background: #0f0f1a;
            border-radius: 6px;
            padding: 16px;
            margin-top: 16px;
            overflow-x: auto;
        }
        .diagram-box pre {
            font-family: 'SF Mono', 'Fira Code', Menlo, Consolas, monospace;
            font-size: 13px;
            line-height: 1.5;
            color: #a78bfa;
            white-space: pre;
        }

        /* Week Tiles */
        .week-grid {
            display: grid;
            grid-template-columns: repeat(auto-fill, minmax(300px, 1fr));
            gap: 16px;
            margin-bottom: 32px;
        }
        .week-tile {
            background: #202020;
            border: 1px solid #2d2d2d;
            border-radius: 12px;
            padding: 20px;
            transition: border-color 0.2s, box-shadow 0.2s;
        }
        .week-tile:hover {
            border-color: var(--accent);
            box-shadow: 0 0 20px rgba(139, 92, 246, 0.08);
        }
        .week-tile.week-coming {
            opacity: 0.5;
        }
        .week-header {
            display: flex;
            justify-content: space-between;
            align-items: center;
            margin-bottom: 8px;
        }
        .week-number {
            font-size: 11px;
            font-weight: 700;
            letter-spacing: 1.2px;
            color: var(--accent);
        }
        .week-status {
            font-size: 11px;
            font-weight: 600;
            padding: 2px 8px;
            border-radius: 4px;
            background: rgba(139, 92, 246, 0.1);
            color: var(--accent);
        }
        .week-title {
            font-size: 18px;
            font-weight: 700;
            color: #e0e0e0;
            margin-bottom: 12px;
        }
        .week-progress-bar {
            height: 4px;
            background: #2d2d2d;
            border-radius: 2px;
            overflow: hidden;
            margin-bottom: 6px;
        }
        .week-progress-fill {
            height: 100%;
            border-radius: 2px;
            transition: width 0.3s;
        }
        .week-progress-text {
            font-size: 12px;
            color: #525252;
            margin-bottom: 14px;
        }
        .week-topics {
            display: flex;
            flex-direction: column;
            gap: 4px;
        }
        .week-topic {
            display: block;
            font-size: 13px;
            color: #9ca3af;
            padding: 6px 10px;
            border-radius: 6px;
            cursor: pointer;
            text-decoration: none;
            transition: background 0.15s, color 0.15s;
        }
        .week-topic:hover {
            background: #2d2d2d;
            color: #e0e0e0;
        }
        .week-topic.locked {
            color: #3d3d3d;
            cursor: default;
        }
        .week-topic.locked:hover {
            background: transparent;
            color: #3d3d3d;
        }

        /* Section Divider */
        .section-divider {
            text-align: center;
            margin: 8px 0 24px;
            position: relative;
        }
        .section-divider::before {
            content: '';
            position: absolute;
            top: 50%;
            left: 0;
            right: 0;
            height: 1px;
            background: #2d2d2d;
        }
        .section-divider span {
            position: relative;
            background: #191919;
            padding: 0 16px;
            font-size: 11px;
            font-weight: 700;
            letter-spacing: 1.2px;
            color: #525252;
        }

        /* Pipeline Steps */
        .pipeline-steps {
            display: flex;
            flex-direction: column;
            gap: 0;
        }
        .pipe-step {
            display: flex;
            align-items: center;
            gap: 16px;
            padding: 16px 20px;
            background: #202020;
            border: 1px solid #2d2d2d;
            border-radius: 12px;
            transition: transform 0.15s, box-shadow 0.15s;
        }
        .pipe-step {
            cursor: pointer;
        }
        .pipe-step:hover {
            transform: translateX(4px);
            box-shadow: 0 2px 12px rgba(139, 92, 246, 0.15);
            border-color: #3d3d3d;
        }
        .pipe-step.final {
            background: linear-gradient(135deg, #1a1a2e 0%, #2d1a4e 100%);
            border-color: #7c3aed;
        }
        .pipe-icon {
            width: 44px;
            height: 44px;
            min-width: 44px;
            border-radius: 10px;
            display: flex;
            align-items: center;
            justify-content: center;
            font-size: 20px;
        }
        .pipe-content {
            flex: 1;
        }
        .pipe-label {
            font-size: 10px;
            font-weight: 700;
            letter-spacing: 1.2px;
            color: #525252;
            margin-bottom: 2px;
        }
        .pipe-title {
            font-size: 15px;
            font-weight: 600;
            color: #e0e0e0;
            margin-bottom: 2px;
        }
        .pipe-desc {
            font-size: 13px;
            color: #6b7280;
            line-height: 1.4;
        }
        .pipe-output {
            font-size: 12px;
            font-weight: 600;
            color: #a78bfa;
            white-space: nowrap;
            padding: 4px 10px;
            background: #1a1a2e;
            border-radius: 6px;
            border: 1px solid #2d2d4e;
        }
        .pipe-connector {
            width: 2px;
            height: 12px;
            background: #3d3d3d;
            margin: 0 0 0 37px;
        }

        /* Key Terms */
        .search-input {
            width: 100%;
            padding: 10px 14px;
            background: #202020;
            border: 1px solid #2d2d2d;
            border-radius: 6px;
            color: #e0e0e0;
            font-size: 14px;
            outline: none;
            margin-bottom: 20px;
        }
        .search-input:focus { border-color: #8b5cf6; }
        .search-input::placeholder { color: #525252; }

        .term-row {
            padding: 12px 0;
            border-bottom: 1px solid #2d2d2d;
            display: grid;
            grid-template-columns: 180px 1fr;
            gap: 12px;
            align-items: baseline;
        }
        .term-row.hidden { display: none; }
        .term-name {
            font-size: 14px;
            font-weight: 600;
            color: #a78bfa;
        }
        .term-def {
            font-size: 14px;
            color: #9ca3af;
            line-height: 1.5;
        }
        .term-source {
            display: none;
        }

        /* Script */
        .script-box {
            padding: 28px;
            background: #1a1a2e;
            border-radius: 8px;
            border-left: 3px solid #8b5cf6;
        }
        .script-para {
            font-size: 16px;
            line-height: 1.9;
            color: #e0e7ff;
            margin-bottom: 16px;
            font-family: Georgia, 'Times New Roman', serif;
        }
        .script-para:last-child { margin-bottom: 0; }

        /* Share Toast */
        .toast {
            position: fixed;
            bottom: 24px;
            right: 24px;
            background: #7c3aed;
            color: #fff;
            padding: 12px 20px;
            border-radius: 8px;
            font-size: 14px;
            font-weight: 500;
            opacity: 0;
            transform: translateY(10px);
            transition: all 0.3s;
            z-index: 1000;
            pointer-events: none;
        }
        .toast.show {
            opacity: 1;
            transform: translateY(0);
        }

        /* Pixel Art Reveal */
        .pixel-grid {
            display: grid;
            grid-template-columns: repeat(13, 1fr);
            gap: 3px;
            max-width: 480px;
            margin: 24px auto;
            padding: 16px;
            background: #141414;
            border-radius: 12px;
            border: 1px solid #2a2a2a;
        }

        .pixel-cell {
            aspect-ratio: 1;
            border-radius: 3px;
            background: #222;
            transition: transform 0.2s, box-shadow 0.2s;
        }

        .pixel-cell.revealed {
            background: var(--pixel-color);
            box-shadow: 0 0 10px color-mix(in srgb, var(--pixel-color) 40%, transparent);
            cursor: pointer;
        }

        .pixel-cell.revealed:hover {
            transform: scale(1.3);
            box-shadow: 0 0 20px color-mix(in srgb, var(--pixel-color) 60%, transparent);
            z-index: 2;
        }

        /* Progress */
        .pixel-progress {
            max-width: 480px;
            margin: 0 auto 8px;
        }
        .pixel-progress-bar {
            height: 6px;
            background: #222;
            border-radius: 3px;
            overflow: hidden;
        }
        .pixel-progress-fill {
            height: 100%;
            background: linear-gradient(90deg, #a855f7, #3b82f6, #22d3ee);
            border-radius: 3px;
            transition: width 0.3s;
        }
        .pixel-progress-text {
            font-size: 12px;
            color: #525252;
            margin-top: 6px;
            text-align: center;
        }

        .pixel-footer {
            text-align: center;
            margin-top: 24px;
            color: #525252;
            font-size: 13px;
            font-style: italic;
        }

        /* Responsive */
        @media (max-width: 768px) {
            .sidebar { display: none; }
            .content { padding: 20px; }
            .page-title { font-size: 24px; }
            .term-row { grid-template-columns: 1fr; }
        }
    </style>
</head>
<body>
    <div class="layout">

        <!-- Sidebar -->
        <div class="sidebar">
            <div class="sidebar-brand">ğŸ§  <span>AI Learning</span></div>

            <a href="#pipeline" class="sidebar-link" onclick="showSection('pipeline')">ğŸ”— The Full Pipeline</a>
<a href="#lego" class="sidebar-link" onclick="showSection('lego')">ğŸ§± The Build</a>
<div class="sidebar-divider"></div>
<div class="sidebar-section-label">STORIES</div>
<a href="#topic-0" class="sidebar-link" onclick="showSection('topic-0')">1. 4-Step LLM Learning Path</a>
<a href="#topic-1" class="sidebar-link" onclick="showSection('topic-1')">2. How LLMs Are Built + Data Prep</a>
<a href="#topic-2" class="sidebar-link" onclick="showSection('topic-2')">3. Tokenization</a>
<a href="#topic-3" class="sidebar-link" onclick="showSection('topic-3')">4. Transformers</a>
<a href="#topic-4" class="sidebar-link" onclick="showSection('topic-4')">5. Model Architecture</a>
<a href="#topic-5" class="sidebar-link" onclick="showSection('topic-5')">6. Model Training</a>
<a href="#topic-6" class="sidebar-link" onclick="showSection('topic-6')">7. Text Generation</a>
<a href="#topic-7" class="sidebar-link" onclick="showSection('topic-7')">8. Post Training SFT</a>
<div class="sidebar-divider"></div>
<a href="#terms" class="sidebar-link" onclick="showSection('terms')">ğŸ“š Key Terms</a>
<a href="#script" class="sidebar-link" onclick="showSection('script')">ğŸ¤ 2-Min Script</a>


            <div class="sidebar-divider"></div>
            <a href="#" class="sidebar-link share-btn" onclick="shareDashboard(event)">ğŸ“¤ Share This Page</a>

            <div class="sidebar-footer">
                Updated Feb 23, 2026<br>
                8 topics â€¢ 47 terms
            </div>
        </div>

        <!-- Content -->
        <div class="content">

            <!-- Pipeline -->
            <div class="page active" id="pipeline">
                <div class="page-breadcrumb">Overview</div>
                <h1 class="page-title">The Full Pipeline</h1>
                <div class="page-meta">How every LLM is built â€” from raw internet to helpful chatbot</div>

                <!-- Week Tiles -->
                <div class="week-grid">
                    
            <div class="week-tile week-active" style="--accent: #8b5cf6;">
                <div class="week-header">
                    <div class="week-number">WEEK 1</div>
                    <div class="week-status">In Progress</div>
                </div>
                <div class="week-title">How LLMs Are Built</div>
                <div class="week-progress-bar">
                    <div class="week-progress-fill" style="width: 100%; background: #8b5cf6;"></div>
                </div>
                <div class="week-progress-text">7/7 topics</div>
                <div class="week-topics">
                    <a class="week-topic" onclick="navigateTo('topic-0')">4-Step LLM Learning Path</a><a class="week-topic" onclick="navigateTo('topic-1')">How LLMs Are Built + Data Prep</a><a class="week-topic" onclick="navigateTo('topic-2')">Tokenization</a><a class="week-topic" onclick="navigateTo('topic-3')">Transformers</a><a class="week-topic" onclick="navigateTo('topic-4')">Model Architecture</a><a class="week-topic" onclick="navigateTo('topic-5')">Model Training</a><a class="week-topic" onclick="navigateTo('topic-7')">Post Training SFT</a>
                </div>
            </div>
        
            <div class="week-tile week-coming" style="--accent: #3b82f6;">
                <div class="week-header">
                    <div class="week-number">WEEK 2</div>
                    <div class="week-status">Coming Soon</div>
                </div>
                <div class="week-title">RAG & Retrieval</div>
                <div class="week-progress-bar">
                    <div class="week-progress-fill" style="width: 0%; background: #3b82f6;"></div>
                </div>
                <div class="week-progress-text">0/1 topics</div>
                <div class="week-topics">
                    <span class="week-topic locked">Topics coming soon...</span>
                </div>
            </div>
        
                </div>

                <div class="section-divider">
                    <span>THE 6-STEP PIPELINE</span>
                </div>

                <div class="pipeline-steps">
                    
                    <div class="pipe-step" onclick="navigateTo('topic-1')" >
                        <div class="pipe-icon" style="background: #3b82f6;">ğŸŒ</div>
                        <div class="pipe-content">
                            <div class="pipe-label">STEP 1</div>
                            <div class="pipe-title">Crawl the Internet</div>
                            <div class="pipe-desc">2.7B pages â€” Common Crawl vacuums the web</div>
                        </div>
                        <div class="pipe-output">200-400 TB raw</div>
                    </div>
                    <div class="pipe-connector"></div>
        
                    <div class="pipe-step" onclick="navigateTo('topic-1')" >
                        <div class="pipe-icon" style="background: #10b981;">ğŸ§¹</div>
                        <div class="pipe-content">
                            <div class="pipe-label">STEP 2</div>
                            <div class="pipe-title">Clean the Data</div>
                            <div class="pipe-desc">Remove HTML, duplicates, PII, toxic content</div>
                        </div>
                        <div class="pipe-output">44 TB clean (FineWeb)</div>
                    </div>
                    <div class="pipe-connector"></div>
        
                    <div class="pipe-step" onclick="navigateTo('topic-2')" >
                        <div class="pipe-icon" style="background: #f59e0b;">ğŸ”¢</div>
                        <div class="pipe-content">
                            <div class="pipe-label">STEP 3</div>
                            <div class="pipe-title">Tokenize</div>
                            <div class="pipe-desc">Text â†’ numbers using BPE (50K-200K vocab)</div>
                        </div>
                        <div class="pipe-output">15T tokens</div>
                    </div>
                    <div class="pipe-connector"></div>
        
                    <div class="pipe-step" onclick="navigateTo('topic-4')" >
                        <div class="pipe-icon" style="background: #8b5cf6;">ğŸ“</div>
                        <div class="pipe-content">
                            <div class="pipe-label">STEP 4</div>
                            <div class="pipe-title">Embed â†’ Transform â†’ Softmax</div>
                            <div class="pipe-desc">Token IDs â†’ vectors â†’ transformer blocks â†’ probabilities â†’ next token</div>
                        </div>
                        <div class="pipe-output">The architecture</div>
                    </div>
                    <div class="pipe-connector"></div>
        
                    <div class="pipe-step" onclick="navigateTo('topic-5')" >
                        <div class="pipe-icon" style="background: #ef4444;">ğŸ”„</div>
                        <div class="pipe-content">
                            <div class="pipe-label">STEP 5</div>
                            <div class="pipe-title">Pre-Train</div>
                            <div class="pipe-desc">Forward â†’ Loss â†’ Backward â†’ Repeat 1M+ times</div>
                        </div>
                        <div class="pipe-output">$10M-$100M+ ğŸ’¸</div>
                    </div>
                    <div class="pipe-connector"></div>
        
                    <div class="pipe-step" onclick="navigateTo('topic-7')" >
                        <div class="pipe-icon" style="background: #ec4899;">ğŸ“</div>
                        <div class="pipe-content">
                            <div class="pipe-label">STEP 6</div>
                            <div class="pipe-title">Post-Train (SFT + RL)</div>
                            <div class="pipe-desc">14K-100K Q&A pairs â†’ genius learns manners</div>
                        </div>
                        <div class="pipe-output">Helpful + Safe</div>
                    </div>
                    <div class="pipe-connector"></div>
        
                    <div class="pipe-step final">
                        <div class="pipe-icon" style="background: #7c3aed;">ğŸ¤–</div>
                        <div class="pipe-content">
                            <div class="pipe-label">RESULT</div>
                            <div class="pipe-title">ChatGPT / Claude / Llama</div>
                            <div class="pipe-desc">Same recipe â€” some just use bigger LEGO towers</div>
                        </div>
                        <div class="pipe-output">Ship it! ğŸš€</div>
                    </div>
    
                </div>
            </div>

            <!-- LEGO Camera Build -->
            
        <div class="page" id="lego">
            <div class="page-breadcrumb">Visual Map</div>
            <h1 class="page-title">The Build</h1>
            <div class="page-meta">8 / 90 pixels revealed</div>

            <div class="pixel-progress">
                <div class="pixel-progress-bar">
                    <div class="pixel-progress-fill" style="width: 8%;"></div>
                </div>
                <div class="pixel-progress-text">8% complete â€” 82 to go</div>
            </div>

            <div class="pixel-grid">
                <div class="pixel-cell"></div>
<div class="pixel-cell revealed" style="--pixel-color: #a855f7;" onclick="navigateTo('topic-4')" title="Model Architecture"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell revealed" style="--pixel-color: #22d3ee;" onclick="navigateTo('topic-7')" title="Post Training SFT"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell revealed" style="--pixel-color: #22d3ee;" onclick="navigateTo('topic-5')" title="Model Training"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell revealed" style="--pixel-color: #a855f7;" onclick="navigateTo('topic-6')" title="Text Generation"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell revealed" style="--pixel-color: #22d3ee;" onclick="navigateTo('topic-0')" title="4-Step LLM Learning Path"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell revealed" style="--pixel-color: #a855f7;" onclick="navigateTo('topic-3')" title="Transformers"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell revealed" style="--pixel-color: #a855f7;" onclick="navigateTo('topic-1')" title="How LLMs Are Built + Data Prep"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell revealed" style="--pixel-color: #a855f7;" onclick="navigateTo('topic-2')" title="Tokenization"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>

            </div>

            <div class="pixel-footer">
                <p>Every topic reveals a pixel. What's the image? Keep learning to find out.</p>
            </div>
        </div>
    

            <!-- Story Pages -->
            
        <div class="page" id="topic-0">
            <a class="back-link" href="#" onclick="navigateTo('pipeline'); return false;">â† Back to Pipeline</a>
            <div class="page-breadcrumb">Chapter 1 of 8</div>
            <h1 class="page-title">4-Step LLM Learning Path</h1>
            <div class="page-meta">Starting point â€” the roadmap</div>

            <div class="content-block kid-block">
                <div class="block-label">ğŸ“– Explain It Like I'm 10</div>
                <p class="story-para">Imagine you want to build a house. Would you start by putting up the roof? No way! You'd start with the foundation, then walls, then the roof, and finally the furniture inside. ğŸ—ï¸</p>
<p class="story-para">Learning AI works EXACTLY the same way. There are 4 steps, and you MUST do them in order:</p>
<p class="story-para">Step 1 is learning the basics â€” what is a neural network? How does a machine learn from data? This is your foundation. Skip this, and everything else feels like magic you can't explain. ğŸ§±</p>
<p class="story-para">Step 2 is learning about Transformers â€” the special brain design from 2017 that changed everything. It's like learning how the engine works before you try to build a car. ğŸï¸</p>
<p class="story-para">Step 3 is the fun part â€” pre-training (teaching the brain from scratch, costs $100 MILLION!), fine-tuning (teaching it YOUR specific job, much cheaper), and RAG (giving it a cheat sheet to look things up without retraining). ğŸ“š</p>
<p class="story-para">Step 4 is building real things â€” AI agents that can use tools, browse the web, and actually DO stuff. This is the roof and furniture â€” it only works because you built everything underneath first! ğŸ¤–</p>
<p class="story-para">The secret? All the resources for these 4 steps are FREE. Andrew Ng, Jay Alammar, Hugging Face â€” they all teach this for free online!</p>

            </div>

            <div class="content-block tech-block">
                <div class="block-label">ğŸ¬ 60-Second Technical Version</div>
                <p class="tech-para">There are exactly 4 steps to go from zero to AI engineer, and ORDER MATTERS. Step 1: ML fundamentals (neural networks, gradient descent). Step 2: Transformers and attention. Step 3: Pre-training ($100M+), fine-tuning (cheap), and RAG (no retraining). Step 4: Applications and AI agents. Skip a step and the house falls down.</p>

                <div class="diagram-box">
                    <pre>  ğŸ—ï¸ THE 4-STEP PATH

  Step 1: ML FUNDAMENTALS     ğŸ§± Foundation
       â”‚
  Step 2: TRANSFORMERS        ğŸï¸ Engine
       â”‚
  Step 3: PRE-TRAIN / FINE-TUNE / RAG
       â”‚  $100M    $cheap    $free
       â”‚
  Step 4: AI AGENTS &amp; APPS    ğŸ¤– The Product

  âš ï¸ Skip a step = house falls down</pre>
                </div>
            </div>

            <a class="share-story-btn" href="#" onclick="shareStory(0); return false;">ğŸ“§ Share This Story</a>
        </div>
        
        <div class="page" id="topic-1">
            <a class="back-link" href="#" onclick="navigateTo('pipeline'); return false;">â† Back to Pipeline</a>
            <div class="page-breadcrumb">Chapter 2 of 8</div>
            <h1 class="page-title">How LLMs Are Built + Data Prep</h1>
            <div class="page-meta">Topic 1 â€” zoom into Step 3 of the roadmap</div>

            <div class="content-block kid-block">
                <div class="block-label">ğŸ“– Explain It Like I'm 10</div>
                <p class="story-para">Imagine you want to teach a robot to speak every language in the world. Where do you start? The INTERNET! ğŸŒ</p>
<p class="story-para">First, you send out a web crawler â€” a little spider bot that visits 2.7 BILLION web pages and copies everything it finds. That's 200-400 terabytes of raw messy text. Think of it like vacuuming the entire internet into a giant bag. ğŸ•·ï¸</p>
<p class="story-para">But that bag is FILTHY â€” it's got HTML code, duplicate pages, people's private info, and toxic content. So you run it through a cleaning machine. Common Crawl collects the raw data, and teams like Hugging Face clean it into something called FineWeb â€” 44 terabytes of squeaky clean text. ğŸ§¹</p>
<p class="story-para">Now you feed that clean text into the model and let it train. This is the EXPENSIVE part â€” GPT-3 cost $10 million, GPT-4 cost over $100 million! Thousands of GPUs running for months. ğŸ’¸</p>
<p class="story-para">But here's the funny part â€” after all that training, the model is like a genius who never learned manners. It knows EVERYTHING but can't hold a conversation! So you do post-training â€” teach it to be helpful and safe. That's much cheaper and faster. ğŸ“</p>
<p class="story-para">And THAT'S how every LLM is built â€” vacuum the internet, clean it, train on it, then teach it manners!</p>

            </div>

            <div class="content-block tech-block">
                <div class="block-label">ğŸ¬ 60-Second Technical Version</div>
                <p class="tech-para">Every LLM starts with the internet. Crawl 2.7B pages (Common Crawl, 200-400 TB raw). Clean it â€” remove HTML, duplicates, PII, toxic content (FineWeb = 44 TB clean, 15 trillion tokens). Tokenize text into numbers. Pretrain ($10M-$100M+, months, thousands of GPUs). Result: a genius with no manners. Post-train to make it helpful and safe. Deploy.</p>

                <div class="diagram-box">
                    <pre>  ğŸŒ Internet (2.7B pages)
       â”‚
  â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
  â”‚  CRAWL     â”‚ â”€â”€â–¶ 200-400 TB raw
  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
  â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
  â”‚  CLEAN     â”‚ â”€â”€â–¶ 44 TB (FineWeb)
  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
  â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
  â”‚  PRETRAIN  â”‚ â”€â”€â–¶ ğŸ’¸ $10M-$100M+
  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
  â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
  â”‚ POST-TRAIN â”‚ â”€â”€â–¶ Genius â†’ Helpful ğŸ“
  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â–¼
  ğŸ¤– Ready to chat!</pre>
                </div>
            </div>

            <a class="share-story-btn" href="#" onclick="shareStory(1); return false;">ğŸ“§ Share This Story</a>
        </div>
        
        <div class="page" id="topic-2">
            <a class="back-link" href="#" onclick="navigateTo('pipeline'); return false;">â† Back to Pipeline</a>
            <div class="page-breadcrumb">Chapter 3 of 8</div>
            <h1 class="page-title">Tokenization</h1>
            <div class="page-meta">Topic 2 â€” data is clean, now convert text â†’ numbers</div>

            <div class="content-block kid-block">
                <div class="block-label">ğŸ“– Explain It Like I'm 10</div>
                <p class="story-para">Here's a problem: computers don't understand words. They ONLY understand numbers. So how do you get a computer to read "Tell me a joke"? You need a translator! That translator is called a Tokenizer. ğŸ”¢</p>
<p class="story-para">The tokenizer has a dictionary â€” a vocabulary â€” with every word-piece it knows and a number for each one. "Tell" = 41, "me" = 553, "a" = 264, "joke" = 22691. Now the computer can work with it!</p>
<p class="story-para">But how big should the dictionary be? If you use WHOLE WORDS, your dictionary needs 500,000+ entries (every word on the internet!). That's way too big. If you use SINGLE LETTERS, the dictionary is tiny (just 26 letters!) but "Hello" becomes 5 tokens instead of 1 â€” everything takes forever. âŒ</p>
<p class="story-para">The sweet spot? Sub-word tokenization! Break words into PIECES. "unhappiness" becomes "un" + "happiness". Common words stay whole, rare words get split up. Dictionary size: 50,000 to 200,000 entries. Perfect balance! âœ…</p>
<p class="story-para">The most popular way to build this dictionary is called BPE â€” Byte Pair Encoding. It starts with individual letters, then keeps merging the most popular pairs together. "h"+"u"+"g" â†’ "hu"+"g" â†’ "hug". Like building bigger LEGO pieces from smaller ones until you have the right set! ğŸ§±</p>
<p class="story-para">Fun fact: type gibberish like "xqzplm" and it explodes into tiny tokens. Type "the cat sat" and it's just 3 tokens. The tokenizer knows common words!</p>

            </div>

            <div class="content-block tech-block">
                <div class="block-label">ğŸ¬ 60-Second Technical Version</div>
                <p class="tech-para">Computers only understand numbers. The tokenizer converts text â†” numbers using a vocabulary (50K-200K entries). Word-level = too big (500K+). Character-level = too long (6.5x). Sub-word (BPE) = perfect balance. BPE algorithm: start with characters, count pairs, merge most frequent, repeat until target vocab size. GPT-3.5 uses ~100K tokens, GPT-4o uses ~200K. Same text = different token IDs across models.</p>

                <div class="diagram-box">
                    <pre>  "Tell me a joke"
       â”‚
  â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ TEXT SPLITTING   â”‚ â”€â”€â–¶ [Tell] [me] [a] [joke]
  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ VOCAB LOOKUP    â”‚ â”€â”€â–¶ [41, 553, 264, 22691]
  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â–¼
  Numbers the model can eat! ğŸ½ï¸

  BPE builds vocab:
  letters â†’ merge pairs â†’ merge more â†’ 50K-200K âœ…</pre>
                </div>
            </div>

            <a class="share-story-btn" href="#" onclick="shareStory(2); return false;">ğŸ“§ Share This Story</a>
        </div>
        
        <div class="page" id="topic-3">
            <a class="back-link" href="#" onclick="navigateTo('pipeline'); return false;">â† Back to Pipeline</a>
            <div class="page-breadcrumb">Chapter 4 of 8</div>
            <h1 class="page-title">Transformers</h1>
            <div class="page-meta">Topic 3 â€” tokens exist, now what brain design processes them?</div>

            <div class="content-block kid-block">
                <div class="block-label">ğŸ“– Explain It Like I'm 10</div>
                <p class="story-para">In 2017, a team at Google published a paper called "Attention Is All You Need" and changed EVERYTHING. Before this, AI read text one word at a time, like reading through a tiny keyhole â€” slow and forgetful. ğŸ”‘</p>
<p class="story-para">The Transformer reads ALL words at once! And it has a superpower called "attention" â€” it figures out which words are important to each other. Read this: "The cat sat on the mat because IT was tired." What does "it" mean? The cat or the mat? YOU know it's the cat. The Transformer figures this out too â€” by letting every word "look at" every other word and decide what matters. That's attention! ğŸ‘ï¸</p>
<p class="story-para">The original Transformer had two halves: an Encoder (reads and understands text) and a Decoder (generates new text). It was built for translation â€” French in, English out. ğŸ‡«ğŸ‡·â¡ï¸ğŸ‡¬ğŸ‡§</p>
<p class="story-para">But then researchers made a wild discovery: throw away the encoder, keep ONLY the decoder, and you get something that's AMAZING at generating text. One word at a time, predicting what comes next. This is called a "decoder-only" transformer. ğŸ”®</p>
<p class="story-para">And here's the punchline â€” ChatGPT, Claude, Llama, Gemini â€” EVERY modern AI chatbot uses this exact same decoder-only design. The 2017 paper literally changed the world! ğŸŒ</p>

            </div>

            <div class="content-block tech-block">
                <div class="block-label">ğŸ¬ 60-Second Technical Version</div>
                <p class="tech-para">In 2017, Google Brain published "Attention Is All You Need" â€” originally for machine translation (French â†’ English). The Transformer reads all words at once using attention (each word looks at every other word to understand context). It had two halves: Encoder (reads input) and Decoder (generates output). Researchers discovered: keep only the decoder = text generation powerhouse. GPT, Claude, Llama = ALL decoder-only transformers. Jay Alammar's "Illustrated Transformer" is the best guide.</p>

                <div class="diagram-box">
                    <pre>  2017: "Attention Is All You Need" (Google)

  ORIGINAL TRANSFORMER:
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ ENCODER  â”‚ DECODER  â”‚  â† Translation
  â”‚ (read)   â”‚ (write)  â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

  MODERN LLMs:
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ DECODER  â”‚  â† Text generation
  â”‚  ONLY    â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  GPT, Claude, Llama = ALL this â˜ï¸

  ğŸ”‘ Attention = words "look at" each other</pre>
                </div>
            </div>

            <a class="share-story-btn" href="#" onclick="shareStory(3); return false;">ğŸ“§ Share This Story</a>
        </div>
        
        <div class="page" id="topic-4">
            <a class="back-link" href="#" onclick="navigateTo('pipeline'); return false;">â† Back to Pipeline</a>
            <div class="page-breadcrumb">Chapter 5 of 8</div>
            <h1 class="page-title">Model Architecture</h1>
            <div class="page-meta">Topics 3 + 4 â€” combine tokens + transformers into the full pipeline</div>

            <div class="content-block kid-block">
                <div class="block-label">ğŸ“– Explain It Like I'm 10</div>
                <p class="story-para">Imagine you're playing a word-guessing game with a really smart robot. You say "I hope you are" and the robot has to guess what comes next. ğŸ¤–</p>
<p class="story-para">But here's the thing â€” the robot can't read words! It only understands numbers. So first, a helper called the Tokenizer chops your words into secret number codes. "I" becomes 42, "hope" becomes 891, and so on.</p>
<p class="story-para">But plain numbers aren't enough. The robot needs RICHER numbers â€” like a whole list of clues for each word. So the Embedding Layer turns each number code into a long list of clues. Think of it like turning a name tag into a full personality profile! ğŸ·ï¸â¡ï¸ğŸ“‹</p>
<p class="story-para">Now comes the big brain part. Those clue-lists pass through a bunch of Transformer Blocks â€” like a stack of really smart filters. Each filter makes the clues better and better. GPT-2 has 12 filters. GPT-3 has 96. Same type of filter, just MORE of them â€” like building a taller LEGO tower! ğŸ§±</p>
<p class="story-para">Finally, the robot looks at the LAST clue-list and asks: "Out of all 50,000 words I know, which one should come next?" It gives every word a score. "well" gets 86 points. "happy" gets 11. "banana" gets 0. Highest score wins! ğŸ†</p>
<p class="story-para">And that's it â€” that's how ChatGPT, Claude, and every AI chatbot works. Same recipe, just some use bigger LEGO towers than others!</p>

            </div>

            <div class="content-block tech-block">
                <div class="block-label">ğŸ¬ 60-Second Technical Version</div>
                <p class="tech-para">Text flows through 4 stages â€” like a Pega case flow. First the Tokenizer chops text into IDs. Then the Embedding Layer turns IDs into vectors (learned during training). Those vectors flow through stacked Transformer Blocks (12 in GPT-2, 96 in GPT-3). Only the LAST vector survives â€” it hits Linear + Softmax, producing 50,000 probabilities. Highest wins. Same architecture for GPT-2 (1.5B), GPT-3 (175B), Llama-3 (405B) â€” different hyperparameters (layers + dimension).</p>

                <div class="diagram-box">
                    <pre> "I hope you are"
       â”‚
  â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”
  â”‚ TOKENIZER â”‚ â”€â”€â–¶ [42, 891, 67, 55]
  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
  â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”
  â”‚ EMBEDDING â”‚ â”€â”€â–¶ [0.2, 0.8, ...] vectors
  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
  â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ TRANSFORMER BLOCKS â”‚ â”€â”€â–¶ keep LAST vector only
  â”‚ (96 in GPT-3!)    â”‚
  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ LINEAR+SOFTMAX â”‚ â”€â”€â–¶ "well" (86%) ğŸ†
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜</pre>
                </div>
            </div>

            <a class="share-story-btn" href="#" onclick="shareStory(4); return false;">ğŸ“§ Share This Story</a>
        </div>
        
        <div class="page" id="topic-5">
            <a class="back-link" href="#" onclick="navigateTo('pipeline'); return false;">â† Back to Pipeline</a>
            <div class="page-breadcrumb">Chapter 6 of 8</div>
            <h1 class="page-title">Model Training</h1>
            <div class="page-meta">Topic 5 â€” architecture is set, now how does it LEARN?</div>

            <div class="content-block kid-block">
                <div class="block-label">ğŸ“– Explain It Like I'm 10</div>
                <p class="story-para">Imagine you build a robot that finishes your sentences. You say "peanut butter and..." and it should say "jelly!" But when you first turn it on, it says "elephant!" because its brain is full of random nonsense. ğŸ¤–</p>
<p class="story-para">So here's how you teach it. You grab a sentence from a book â€” "The cat sat on the..." â€” and you KNOW the answer is "mat." You let the robot guess. It says "pizza!" with 15% confidence. Then you use a GRADE MACHINE (that's the loss function) â€” it compares "pizza" with "mat" and gives a grade: 3.0. In this game, high is BAD â€” like golf! â›³</p>
<p class="story-para">Then a HELPER ROBOT (that's the optimizer) looks at the grade, goes into the robot's brain, and turns billions of tiny knobs so that "mat" gets a little more likely and "pizza" gets a little less likely next time. ğŸ”§</p>
<p class="story-para">One round doesn't fix it. But you do this a MILLION times with different sentences. Slowly, the robot stops saying "elephant" and starts saying "jelly." Not because anyone TOLD it peanut butter goes with jelly â€” it figured it out from seeing the pattern thousands of times! That's implicit knowledge. ğŸ§ </p>
<p class="story-para">The code for this? About 10 lines. But running it on 405 billion knobs? That needs 16,000 of the world's best computers running for weeks, costing millions of dollars. Simple recipe, enormous kitchen! ğŸ—ï¸</p>

            </div>

            <div class="content-block tech-block">
                <div class="block-label">ğŸ¬ 60-Second Technical Version</div>
                <p class="tech-para">Training is a 2-step dance repeated a million times. Step 1: feed text into the LLM, get probabilities, grade them with cross-entropy loss â€” just one number, high means bad guess. Step 2: the optimizer takes that grade, goes backward through every layer, and nudges every weight so the next guess is slightly better. After a million rounds across the internet, the model discovers patterns nobody taught it â€” Einstein connects to physicist, machine learning connects to algorithms, not bananas. That's implicit knowledge. The code is 10 lines. Running it on Llama 3's 405 billion parameters takes 16,000 H100 GPUs, 7+ TB of memory, and millions of dollars. Simple recipe, massive kitchen.</p>

                <div class="diagram-box">
                    <pre>  ğŸŒ Internet Text (cleaned, tokenized)
       â”‚
       â–¼  Sample: "Einstein was a German-born ___"
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ STEP 1: FORWARD PASS + LOSS        â”‚
  â”‚ Input â†’ LLM â†’ probabilities        â”‚
  â”‚ Compare with correct ("physicist") â”‚
  â”‚ Loss = 3.0 (high = bad) ğŸ“›         â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ STEP 2: OPTIMIZER (backward pass)  â”‚
  â”‚ Takes loss â†’ adjusts ALL weights   â”‚
  â”‚ "physicist" prob: 0.05 â†’ 0.06     â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â–¼  ğŸ”„ REPEAT 1,000,000+ times
       ğŸ† Implicit knowledge acquired!</pre>
                </div>
            </div>

            <a class="share-story-btn" href="#" onclick="shareStory(5); return false;">ğŸ“§ Share This Story</a>
        </div>
        
        <div class="page" id="topic-6">
            <a class="back-link" href="#" onclick="navigateTo('pipeline'); return false;">â† Back to Pipeline</a>
            <div class="page-breadcrumb">Chapter 7 of 8</div>
            <h1 class="page-title">Text Generation</h1>
            <div class="page-meta">Topic 6 â€” model is trained, now how does it PRODUCE text?</div>

            <div class="content-block kid-block">
                <div class="block-label">ğŸ“– Explain It Like I'm 10</div>
                <p class="story-para">Imagine you have a super smart robot that knows every word in every book ever written. You ask it to finish your sentence: "Once upon a..." But the robot doesn't just KNOW the answer â€” it has a list of 50,000 possible next words, each with a score of how likely it is. ğŸ¤–</p>
<p class="story-para">So how does it pick? The lazy way (Greedy Search) is to always pick the word with the highest score. "time" wins at 40%! Easy! But here's the problem â€” the robot gets stuck in a loop. Like a kid who tells the same joke over and over: "and then... and then... and then..." It picks the same popular words FOREVER. ğŸ”„</p>
<p class="story-para">So smart scientists said: "What if we let the robot be a LITTLE random?" That's Top-P Sampling. Instead of picking just the #1 word, you grab all the top words until their scores add up to 90%. If the robot is SURE the next word is "time" (89% confident), it only looks at that one word. But if it's unsure â€” maybe "time" is 30%, "a" is 25%, "the" is 20% â€” it grabs ALL of those and randomly picks from them. The robot adapts! Confident = focused. Unsure = explore! ğŸ¯</p>
<p class="story-para">And there's a temperature knob. Turn it DOWN for homework (be precise!). Turn it UP for storytelling (be wild!). That's why when you ask ChatGPT to write code, it's careful and exact. But when you ask it to write a poem, it surprises you every time. ğŸŒ¡ï¸</p>
<p class="story-para">The whole thing is just a for loop: pick a word, add it to the sentence, pick the next word, repeat. Stop when the robot says "THE END" or you hit a word limit. That's it â€” that's how every AI writes! âœ¨</p>

            </div>

            <div class="content-block tech-block">
                <div class="block-label">ğŸ¬ 60-Second Technical Version</div>
                <p class="tech-para">Here's something most people don't realize â€” after all that expensive training ($100M+, months of GPU time), the model doesn't actually 'know' what to say. It knows probabilities. You type 'Albert' and the model runs a forward pass through its billions of parameters and outputs a probability distribution over its entire vocabulary â€” roughly 50,000 possible next tokens. 'Einstein' might get 42%, 'John' gets 15%, 'was' gets 8%, and so on. The question is: how do you pick from that list?</p>
<p class="tech-para">The naive approach is Greedy Search â€” always pick the highest probability token. Simple, fast, but fatally flawed. It causes repetitive loops because the model keeps choosing the same 'safe' high-probability words. OpenAI demonstrated this with GPT-2: ask it about a cute dog and it literally gets stuck repeating the same sentence. Beam Search improved on this by tracking K parallel paths (like exploring K branches of a decision tree simultaneously), but it's still deterministic â€” same input always produces the same output, which makes the text feel robotic.</p>
<p class="tech-para">The real breakthrough was moving from deterministic to stochastic (random) sampling. Top-K Sampling picks randomly from the K most likely tokens â€” but K is fixed, which is a problem. Sometimes the model is very confident (one token has 90% probability) and you want to focus. Other times it's unsure and you want to explore. Top-P (Nucleus) Sampling solved this elegantly: instead of a fixed K, you keep adding tokens from highest to lowest probability until their cumulative probability exceeds a threshold P (usually 0.9). This means K is DYNAMIC â€” when confident, only 1-2 tokens qualify. When uncertain, maybe 20 tokens qualify. The model naturally adapts its creativity based on context.</p>
<p class="tech-para">Finally, Temperature acts as a creativity dial on TOP of the sampling algorithm. Low temperature (0.1-0.3) sharpens the probability distribution â€” the rich get richer, making output precise and focused (ideal for code generation). High temperature (0.7-1.0) flattens it â€” giving underdog tokens a fighting chance, making output more creative and surprising (ideal for storytelling, poetry). Every time you adjust that 'temperature' slider in ChatGPT or Claude, this is exactly what's happening.</p>
<p class="tech-para">The entire generation process is surprisingly simple in code â€” it's a for loop. Feed in the prompt, get probabilities, sample one token, append it to the input, repeat. Stop when the model outputs a special &lt;EOS&gt; (End of Sequence) token, or when you hit a maximum length. That's it. Every AI response you've ever read was generated exactly this way â€” one token at a time, in a loop.</p>

                <div class="diagram-box">
                    <pre>  HOW EVERY AI RESPONSE IS GENERATED
  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  Input: "Albert"        (your prompt)
       â–¼
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚   LLM   â”‚â”€â”€â”€â–¶â”‚ 50K probs    â”‚â”€â”€â”€â–¶â”‚ SAMPLING â”‚
  â”‚(forward  â”‚    â”‚ Einstein:42% â”‚    â”‚ALGORITHM â”‚
  â”‚  pass)   â”‚    â”‚ John: 15%    â”‚    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚ was: 8%      â”‚         â”‚
       â–²         â”‚ the: 5%      â”‚         â–¼
       â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   "Einstein"
       â”‚                                  â”‚
       â””â”€â”€â”€â”€â”€ append to input â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       ğŸ”„ REPEAT until &lt;EOS&gt; or max length
  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  THE EVOLUTION OF SAMPLING:
  âŒ Greedy    â†’ always pick #1 â†’ repetition loops
  âŒ Beam(K=5) â†’ track 5 paths  â†’ still deterministic
  âš ï¸ Top-K     â†’ random from top K â†’ but K is fixed
  âœ… Top-P â­  â†’ dynamic K based on confidence!
     Confident (90% on one token) â†’ K=1, focused
     Uncertain (spread across 20)  â†’ K=20, explore
  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  ğŸŒ¡ï¸ TEMPERATURE (the creativity dial):
     Low (0.1)  â†’ sharp probs â†’ precise (code)
     High (0.9) â†’ flat probs  â†’ creative (poetry)</pre>
                </div>
            </div>

            <a class="share-story-btn" href="#" onclick="shareStory(6); return false;">ğŸ“§ Share This Story</a>
        </div>
        
        <div class="page" id="topic-7">
            <a class="back-link" href="#" onclick="navigateTo('pipeline'); return false;">â† Back to Pipeline</a>
            <div class="page-breadcrumb">Chapter 8 of 8</div>
            <h1 class="page-title">Post Training SFT</h1>
            <div class="page-meta">Topic 7 â€” model generates text, now make it actually useful</div>

            <div class="content-block kid-block">
                <div class="block-label">ğŸ“– Explain It Like I'm 10</div>
                <p class="story-para">Remember that genius who never learned manners? The one who read the entire internet but can't hold a conversation? That's a base model after pre-training. It can finish any sentence, but ask it "What's the capital of France?" and it might just keep rambling instead of answering. ğŸ¤·</p>
<p class="story-para">SFT â€” Supervised Fine-Tuning â€” is like sending that genius to finishing school. You give it 14,000 to 100,000 examples of perfect conversations: "Question: What's 2+2? Answer: 4." Over and over, until it learns the FORMAT of being helpful. ğŸ“</p>
<p class="story-para">Here's the wild part â€” the training algorithm is IDENTICAL to pre-training! Same math, same optimizer, same loop. The ONLY thing that changes is the DATA. Instead of messy internet text, you feed it carefully curated question-answer pairs written by experts. Quality over quantity! âœ¨</p>
<p class="story-para">After SFT, the model can answer questions. But it's still not safe â€” it might help you do dangerous things if you ask nicely. That's where step 2 comes in: Reinforcement Learning (RL) teaches it to be safe and honest. ğŸ›¡ï¸</p>
<p class="story-para">Think of it like Pega: Pre-training = installing the platform. SFT = configuring it for your business. RL = user testing to catch edge cases. Same platform, just refined step by step!</p>

            </div>

            <div class="content-block tech-block">
                <div class="block-label">ğŸ¬ 60-Second Technical Version</div>
                <p class="tech-para">SFT transforms a base model (genius with no manners) into a question-answering assistant. Feed it 14K-100K expert-curated instruction-response pairs. The training algorithm is IDENTICAL to pre-training â€” only the data format changes. Pre-training: trillions of tokens, $100M. SFT: thousands of examples, much cheaper. Result: model follows instructions. Then RL makes it safe/honest. Popular datasets: InstructGPT (14.5K), Alpaca (52K), Dolly, Flan.</p>

                <div class="diagram-box">
                    <pre>  BASE MODEL (genius, no manners)
       â”‚
  â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ SFT: Supervised        â”‚
  â”‚ Fine-Tuning            â”‚
  â”‚                        â”‚
  â”‚ Data: 14K-100K pairs   â”‚
  â”‚ "Q: ..." â†’ "A: ..."   â”‚
  â”‚ Same algorithm!        â”‚
  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
  â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ RL: Reinforcement      â”‚
  â”‚ Learning (safe+honest) â”‚
  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â–¼
  ğŸ¤– Helpful + Safe Assistant</pre>
                </div>
            </div>

            <a class="share-story-btn" href="#" onclick="shareStory(7); return false;">ğŸ“§ Share This Story</a>
        </div>
        

            <!-- Key Terms -->
            <div class="page" id="terms">
                <div class="page-breadcrumb">Reference</div>
                <h1 class="page-title">Key Terms</h1>
                <div class="page-meta">47 terms across all topics</div>

                <input type="text" class="search-input" placeholder="Search terms..." oninput="filterTerms(this.value)">
                
        <div class="term-row" data-search="neural network a math formula that learns patterns from data">
            <div class="term-name">Neural Network</div>
            <div class="term-def">A math formula that learns patterns from data</div>
            <div class="term-source">The 4-Step Learning Path</div>
        </div>
        
        <div class="term-row" data-search="transformer the brain architecture behind every modern ai (invented 2017)">
            <div class="term-name">Transformer</div>
            <div class="term-def">The brain architecture behind every modern AI (invented 2017)</div>
            <div class="term-source">The 4-Step Learning Path</div>
        </div>
        
        <div class="term-row" data-search="pre-training teaching ai from scratch using the internet ($100m+)">
            <div class="term-name">Pre-training</div>
            <div class="term-def">Teaching AI from scratch using the internet ($100M+)</div>
            <div class="term-source">The 4-Step Learning Path</div>
        </div>
        
        <div class="term-row" data-search="fine-tuning teaching it your specific job (much cheaper)">
            <div class="term-name">Fine-tuning</div>
            <div class="term-def">Teaching it YOUR specific job (much cheaper)</div>
            <div class="term-source">The 4-Step Learning Path</div>
        </div>
        
        <div class="term-row" data-search="rag giving the ai a reference book to look things up (no retraining needed)">
            <div class="term-name">RAG</div>
            <div class="term-def">Giving the AI a reference book to look things up (no retraining needed)</div>
            <div class="term-source">The 4-Step Learning Path</div>
        </div>
        
        <div class="term-row" data-search="common crawl free public dataset that crawls the internet every 1-2 months">
            <div class="term-name">Common Crawl</div>
            <div class="term-def">Free public dataset that crawls the internet every 1-2 months</div>
            <div class="term-source">How LLMs Are Built + Data Prep</div>
        </div>
        
        <div class="term-row" data-search="fineweb hugging face's cleaned dataset (44 tb, 15 trillion tokens)">
            <div class="term-name">FineWeb</div>
            <div class="term-def">Hugging Face's cleaned dataset (44 TB, 15 trillion tokens)</div>
            <div class="term-source">How LLMs Are Built + Data Prep</div>
        </div>
        
        <div class="term-row" data-search="pii removal stripping out personal info (names, emails, phone numbers)">
            <div class="term-name">PII removal</div>
            <div class="term-def">Stripping out personal info (names, emails, phone numbers)</div>
            <div class="term-source">How LLMs Are Built + Data Prep</div>
        </div>
        
        <div class="term-row" data-search="base model the "genius with no manners" after pre-training">
            <div class="term-name">Base model</div>
            <div class="term-def">The "genius with no manners" after pre-training</div>
            <div class="term-source">How LLMs Are Built + Data Prep</div>
        </div>
        
        <div class="term-row" data-search="post-training teaching the genius to be helpful and safe">
            <div class="term-name">Post-training</div>
            <div class="term-def">Teaching the genius to be helpful and safe</div>
            <div class="term-source">How LLMs Are Built + Data Prep</div>
        </div>
        
        <div class="term-row" data-search="tokenizer converts text â†” numbers">
            <div class="term-name">Tokenizer</div>
            <div class="term-def">Converts text â†” numbers</div>
            <div class="term-source">Tokenization</div>
        </div>
        
        <div class="term-row" data-search="vocabulary the dictionary of all known word-pieces and their ids">
            <div class="term-name">Vocabulary</div>
            <div class="term-def">The dictionary of all known word-pieces and their IDs</div>
            <div class="term-source">Tokenization</div>
        </div>
        
        <div class="term-row" data-search="bpe (byte pair encoding) algorithm: start with characters, merge most frequent pairs, repeat">
            <div class="term-name">BPE (Byte Pair Encoding)</div>
            <div class="term-def">Algorithm: start with characters, merge most frequent pairs, repeat</div>
            <div class="term-source">Tokenization</div>
        </div>
        
        <div class="term-row" data-search="sub-word tokenization bigger than characters, smaller than words (the sweet spot)">
            <div class="term-name">Sub-word tokenization</div>
            <div class="term-def">Bigger than characters, smaller than words (the sweet spot)</div>
            <div class="term-source">Tokenization</div>
        </div>
        
        <div class="term-row" data-search="token ids the numbers that represent each piece of text">
            <div class="term-name">Token IDs</div>
            <div class="term-def">The numbers that represent each piece of text</div>
            <div class="term-source">Tokenization</div>
        </div>
        
        <div class="term-row" data-search="attention mechanism lets each word "look at" every other word to understand context">
            <div class="term-name">Attention Mechanism</div>
            <div class="term-def">Lets each word "look at" every other word to understand context</div>
            <div class="term-source">Transformers</div>
        </div>
        
        <div class="term-row" data-search="self-attention words in the same sentence attending to each other">
            <div class="term-name">Self-Attention</div>
            <div class="term-def">Words in the same sentence attending to each other</div>
            <div class="term-source">Transformers</div>
        </div>
        
        <div class="term-row" data-search="encoder reads and understands input (used in bert)">
            <div class="term-name">Encoder</div>
            <div class="term-def">Reads and understands input (used in BERT)</div>
            <div class="term-source">Transformers</div>
        </div>
        
        <div class="term-row" data-search="decoder generates output one token at a time (used in gpt, claude, llama)">
            <div class="term-name">Decoder</div>
            <div class="term-def">Generates output one token at a time (used in GPT, Claude, Llama)</div>
            <div class="term-source">Transformers</div>
        </div>
        
        <div class="term-row" data-search=""attention is all you need" the 2017 paper that started it all (google brain)">
            <div class="term-name">"Attention Is All You Need"</div>
            <div class="term-def">The 2017 paper that started it all (Google Brain)</div>
            <div class="term-source">Transformers</div>
        </div>
        
        <div class="term-row" data-search="jay alammar wrote the famous "illustrated transformer" guide">
            <div class="term-name">Jay Alammar</div>
            <div class="term-def">Wrote the famous "Illustrated Transformer" guide</div>
            <div class="term-source">Transformers</div>
        </div>
        
        <div class="term-row" data-search="embedding layer turns token ids into vectors (rich number-lists the model can work with)">
            <div class="term-name">Embedding Layer</div>
            <div class="term-def">Turns token IDs into vectors (rich number-lists the model can work with)</div>
            <div class="term-source">Model Architecture</div>
        </div>
        
        <div class="term-row" data-search="transformer block a stack of layers (attention + linear + more) that process vectors">
            <div class="term-name">Transformer Block</div>
            <div class="term-def">A stack of layers (attention + linear + more) that process vectors</div>
            <div class="term-source">Model Architecture</div>
        </div>
        
        <div class="term-row" data-search="decoder-only transformer only the decoder half â€” used by all modern llms (gpt, claude, llama)">
            <div class="term-name">Decoder-only Transformer</div>
            <div class="term-def">Only the decoder half â€” used by ALL modern LLMs (GPT, Claude, Llama)</div>
            <div class="term-source">Model Architecture</div>
        </div>
        
        <div class="term-row" data-search="softmax the operation that turns numbers into probabilities (all sum to 1)">
            <div class="term-name">Softmax</div>
            <div class="term-def">The operation that turns numbers into probabilities (all sum to 1)</div>
            <div class="term-source">Model Architecture</div>
        </div>
        
        <div class="term-row" data-search="lm head (linear + softmax) final stage that picks the next token">
            <div class="term-name">LM Head (Linear + Softmax)</div>
            <div class="term-def">Final stage that picks the next token</div>
            <div class="term-source">Model Architecture</div>
        </div>
        
        <div class="term-row" data-search="hyperparameters settings that control model size (layers, dimension)">
            <div class="term-name">Hyperparameters</div>
            <div class="term-def">Settings that control model size (layers, dimension)</div>
            <div class="term-source">Model Architecture</div>
        </div>
        
        <div class="term-row" data-search="forward pass model reads text and guesses the next word">
            <div class="term-name">Forward Pass</div>
            <div class="term-def">Model reads text and guesses the next word</div>
            <div class="term-source">Model Training</div>
        </div>
        
        <div class="term-row" data-search="loss (cross-entropy) how wrong the guess was (3.0 = bad, 0.14 = good)">
            <div class="term-name">Loss (Cross-Entropy)</div>
            <div class="term-def">How wrong the guess was (3.0 = bad, 0.14 = good)</div>
            <div class="term-source">Model Training</div>
        </div>
        
        <div class="term-row" data-search="backward pass (backpropagation) optimizer calculates how to fix each weight">
            <div class="term-name">Backward Pass (Backpropagation)</div>
            <div class="term-def">Optimizer calculates how to fix each weight</div>
            <div class="term-source">Model Training</div>
        </div>
        
        <div class="term-row" data-search="optimizer (adam) the robot that adjusts billions of weights based on loss">
            <div class="term-name">Optimizer (Adam)</div>
            <div class="term-def">The robot that adjusts billions of weights based on loss</div>
            <div class="term-source">Model Training</div>
        </div>
        
        <div class="term-row" data-search="weights/parameters the billions of tiny knobs that determine what the model knows">
            <div class="term-name">Weights/Parameters</div>
            <div class="term-def">The billions of tiny knobs that determine what the model knows</div>
            <div class="term-source">Model Training</div>
        </div>
        
        <div class="term-row" data-search="implicit knowledge model discovers patterns from statistics, not explicit teaching">
            <div class="term-name">Implicit Knowledge</div>
            <div class="term-def">Model discovers patterns from statistics, not explicit teaching</div>
            <div class="term-source">Model Training</div>
        </div>
        
        <div class="term-row" data-search="autoregressive generation predicting one token at a time, feeding it back as input">
            <div class="term-name">Autoregressive Generation</div>
            <div class="term-def">Predicting one token at a time, feeding it back as input</div>
            <div class="term-source">Text Generation</div>
        </div>
        
        <div class="term-row" data-search="greedy search always pick highest probability token (simple but repetitive)">
            <div class="term-name">Greedy Search</div>
            <div class="term-def">Always pick highest probability token (simple but repetitive)</div>
            <div class="term-source">Text Generation</div>
        </div>
        
        <div class="term-row" data-search="beam search track top-k paths simultaneously (better but still deterministic)">
            <div class="term-name">Beam Search</div>
            <div class="term-def">Track top-K paths simultaneously (better but still deterministic)</div>
            <div class="term-source">Text Generation</div>
        </div>
        
        <div class="term-row" data-search="multinomial sampling sample randomly according to probabilities (too risky)">
            <div class="term-name">Multinomial Sampling</div>
            <div class="term-def">Sample randomly according to probabilities (too risky)</div>
            <div class="term-source">Text Generation</div>
        </div>
        
        <div class="term-row" data-search="top-k sampling keep top-k tokens, sample from those (fixed k is the problem)">
            <div class="term-name">Top-K Sampling</div>
            <div class="term-def">Keep top-K tokens, sample from those (fixed K is the problem)</div>
            <div class="term-source">Text Generation</div>
        </div>
        
        <div class="term-row" data-search="top-p sampling (nucleus) keep tokens until cumulative probability > p, then sample (dynamic k â€” used in practice)">
            <div class="term-name">Top-P Sampling (Nucleus)</div>
            <div class="term-def">Keep tokens until cumulative probability > P, then sample (dynamic K â€” USED IN PRACTICE)</div>
            <div class="term-source">Text Generation</div>
        </div>
        
        <div class="term-row" data-search="temperature hyperparameter that sharpens (low) or smooths (high) probabilities">
            <div class="term-name">Temperature</div>
            <div class="term-def">Hyperparameter that sharpens (low) or smooths (high) probabilities</div>
            <div class="term-source">Text Generation</div>
        </div>
        
        <div class="term-row" data-search="deterministic same input â†’ same output always (greedy, beam)">
            <div class="term-name">Deterministic</div>
            <div class="term-def">Same input â†’ same output always (Greedy, Beam)</div>
            <div class="term-source">Text Generation</div>
        </div>
        
        <div class="term-row" data-search="stochastic same input â†’ different outputs each time (top-k, top-p)">
            <div class="term-name">Stochastic</div>
            <div class="term-def">Same input â†’ different outputs each time (Top-K, Top-P)</div>
            <div class="term-source">Text Generation</div>
        </div>
        
        <div class="term-row" data-search="sft (supervised fine-tuning) teaching the base model to follow instructions using q&a pairs">
            <div class="term-name">SFT (Supervised Fine-Tuning)</div>
            <div class="term-def">Teaching the base model to follow instructions using Q&A pairs</div>
            <div class="term-source">Post Training (SFT)</div>
        </div>
        
        <div class="term-row" data-search="base model pre-trained model that knows language but can't chat properly">
            <div class="term-name">Base Model</div>
            <div class="term-def">Pre-trained model that knows language but can't chat properly</div>
            <div class="term-source">Post Training (SFT)</div>
        </div>
        
        <div class="term-row" data-search="instruction-response pairs "q: what is 2+2? a: 4" format training data">
            <div class="term-name">Instruction-Response Pairs</div>
            <div class="term-def">"Q: What is 2+2? A: 4" format training data</div>
            <div class="term-source">Post Training (SFT)</div>
        </div>
        
        <div class="term-row" data-search="rl (reinforcement learning) second stage: teaches model to be safe and honest">
            <div class="term-name">RL (Reinforcement Learning)</div>
            <div class="term-def">Second stage: teaches model to be safe and honest</div>
            <div class="term-source">Post Training (SFT)</div>
        </div>
        
        <div class="term-row" data-search="data quality > quantity 14k perfect examples beat millions of mediocre ones">
            <div class="term-name">Data Quality > Quantity</div>
            <div class="term-def">14K perfect examples beat millions of mediocre ones</div>
            <div class="term-source">Post Training (SFT)</div>
        </div>
        
            </div>

            <!-- 2-Min Script -->
            <div class="page" id="script">
                <div class="page-breadcrumb">Interview Prep</div>
                <h1 class="page-title">Explain It All in 2 Minutes</h1>
                <div class="page-meta">Read this out loud. If you can say it naturally, you own every concept.</div>

                <div class="script-box">
                    <p class="script-para">So you want to know how ChatGPT works? Here's the whole story:</p>
<p class="script-para">First, they vacuumed up the entire internet â€” 2.7 billion web pages â€” and cleaned it. Then they built a translator called a tokenizer that converts words into numbers, because computers can't read text.</p>
<p class="script-para">Those numbers go through 4 stages: the tokenizer chops text into IDs, an embedding layer turns IDs into rich vectors, transformer blocks process those vectors through dozens of smart filters, and a softmax function picks the most likely next word out of 50,000 options.</p>
<p class="script-para">To teach it, they run a loop a million times: guess the next word, check how wrong it was, adjust billions of tiny knobs, repeat. This costs $100 million and takes months on thousands of GPUs.</p>
<p class="script-para">But a trained model only gives you probabilities â€” it doesn't actually write text yet. Text generation is a for loop: pick one token using Top-P sampling (keep the most likely words until they add up to 90%, randomly pick from those), add it to the sentence, repeat. Temperature controls creativity â€” low for code, high for stories.</p>
<p class="script-para">After all that, the model is a genius who can't hold a conversation. So they show it 14,000 perfect Q&amp;A examples (that's fine-tuning), and then use reinforcement learning to make it safe.</p>
<p class="script-para">And that's it â€” that's ChatGPT, Claude, Llama. Same recipe. Some just use bigger LEGO towers than others.</p>

                </div>
            </div>

        </div>
    </div>

    <div class="toast" id="toast"></div>

    <script>
        const storyShareData = [{"topic": "4-Step LLM Learning Path", "kid_story": "Imagine you want to build a house. Would you start by putting up the roof? No way! You'd start with the foundation, then walls, then the roof, and finally the furniture inside. \ud83c\udfd7\ufe0f\n\nLearning AI works EXACTLY the same way. There are 4 steps, and you MUST do them in order:\n\nStep 1 is learning the basics \u2014 what is a neural network? How does a machine learn from data? This is your foundation. Skip this, and everything else feels like magic you can't explain. \ud83e\uddf1\n\nStep 2 is learning about Transformers \u2014"}, {"topic": "How LLMs Are Built + Data Prep", "kid_story": "Imagine you want to teach a robot to speak every language in the world. Where do you start? The INTERNET! \ud83c\udf10\n\nFirst, you send out a web crawler \u2014 a little spider bot that visits 2.7 BILLION web pages and copies everything it finds. That's 200-400 terabytes of raw messy text. Think of it like vacuuming the entire internet into a giant bag. \ud83d\udd77\ufe0f\n\nBut that bag is FILTHY \u2014 it's got HTML code, duplicate pages, people's private info, and toxic content. So you run it through a cleaning machine. Common Cra"}, {"topic": "Tokenization", "kid_story": "Here's a problem: computers don't understand words. They ONLY understand numbers. So how do you get a computer to read \"Tell me a joke\"? You need a translator! That translator is called a Tokenizer. \ud83d\udd22\n\nThe tokenizer has a dictionary \u2014 a vocabulary \u2014 with every word-piece it knows and a number for each one. \"Tell\" = 41, \"me\" = 553, \"a\" = 264, \"joke\" = 22691. Now the computer can work with it!\n\nBut how big should the dictionary be? If you use WHOLE WORDS, your dictionary needs 500,000+ entries (ev"}, {"topic": "Transformers", "kid_story": "In 2017, a team at Google published a paper called \"Attention Is All You Need\" and changed EVERYTHING. Before this, AI read text one word at a time, like reading through a tiny keyhole \u2014 slow and forgetful. \ud83d\udd11\n\nThe Transformer reads ALL words at once! And it has a superpower called \"attention\" \u2014 it figures out which words are important to each other. Read this: \"The cat sat on the mat because IT was tired.\" What does \"it\" mean? The cat or the mat? YOU know it's the cat. The Transformer figures th"}, {"topic": "Model Architecture", "kid_story": "Imagine you're playing a word-guessing game with a really smart robot. You say \"I hope you are\" and the robot has to guess what comes next. \ud83e\udd16\n\nBut here's the thing \u2014 the robot can't read words! It only understands numbers. So first, a helper called the Tokenizer chops your words into secret number codes. \"I\" becomes 42, \"hope\" becomes 891, and so on.\n\nBut plain numbers aren't enough. The robot needs RICHER numbers \u2014 like a whole list of clues for each word. So the Embedding Layer turns each numb"}, {"topic": "Model Training", "kid_story": "Imagine you build a robot that finishes your sentences. You say \"peanut butter and...\" and it should say \"jelly!\" But when you first turn it on, it says \"elephant!\" because its brain is full of random nonsense. \ud83e\udd16\n\nSo here's how you teach it. You grab a sentence from a book \u2014 \"The cat sat on the...\" \u2014 and you KNOW the answer is \"mat.\" You let the robot guess. It says \"pizza!\" with 15% confidence. Then you use a GRADE MACHINE (that's the loss function) \u2014 it compares \"pizza\" with \"mat\" and gives a "}, {"topic": "Text Generation", "kid_story": "Imagine you have a super smart robot that knows every word in every book ever written. You ask it to finish your sentence: \"Once upon a...\" But the robot doesn't just KNOW the answer \u2014 it has a list of 50,000 possible next words, each with a score of how likely it is. \ud83e\udd16\n\nSo how does it pick? The lazy way (Greedy Search) is to always pick the word with the highest score. \"time\" wins at 40%! Easy! But here's the problem \u2014 the robot gets stuck in a loop. Like a kid who tells the same joke over and "}, {"topic": "Post Training SFT", "kid_story": "Remember that genius who never learned manners? The one who read the entire internet but can't hold a conversation? That's a base model after pre-training. It can finish any sentence, but ask it \"What's the capital of France?\" and it might just keep rambling instead of answering. \ud83e\udd37\n\nSFT \u2014 Supervised Fine-Tuning \u2014 is like sending that genius to finishing school. You give it 14,000 to 100,000 examples of perfect conversations: \"Question: What's 2+2? Answer: 4.\" Over and over, until it learns the F"}];

        function shareStory(index) {
            const story = storyShareData[index];
            if (!story) return;
            const subject = encodeURIComponent('Check out this AI story: ' + story.topic);
            const dashboardUrl = 'https://kneil31.github.io/ai-learning/';
            const preview = story.kid_story.substring(0, 300) + (story.kid_story.length > 300 ? '...' : '');
            const body = encodeURIComponent(
                'ğŸ“– ' + story.topic + '\n\n' +
                preview + '\n\n' +
                'ğŸ”— Read the full story (with diagrams): ' + dashboardUrl + '\n\n' +
                'â€” From Ram\'s AI Learning Journey'
            );
            const mailtoLink = 'mailto:?subject=' + subject + '&body=' + body;
            const a = document.createElement('a');
            a.href = mailtoLink;
            a.target = '_blank';
            a.click();
        }

        function showSection(id) {
            // Hide all pages
            document.querySelectorAll('.page').forEach(p => p.classList.remove('active'));
            // Show target
            const target = document.getElementById(id);
            if (target) target.classList.add('active');
            // Update sidebar active state
            document.querySelectorAll('.sidebar-link').forEach(a => a.classList.remove('active'));
            if (event && event.currentTarget) event.currentTarget.classList.add('active');
            // Also highlight matching sidebar link by href
            document.querySelectorAll('.sidebar-link').forEach(a => {
                if (a.getAttribute('href') === '#' + id) a.classList.add('active');
            });
            // Scroll content to top
            document.querySelector('.content').scrollTop = 0;
        }

        function navigateTo(sectionId) {
            // Used by pipeline steps to jump to a story page
            document.querySelectorAll('.page').forEach(p => p.classList.remove('active'));
            const target = document.getElementById(sectionId);
            if (target) target.classList.add('active');
            // Update sidebar
            document.querySelectorAll('.sidebar-link').forEach(a => {
                a.classList.remove('active');
                if (a.getAttribute('href') === '#' + sectionId) a.classList.add('active');
            });
            document.querySelector('.content').scrollTop = 0;
        }

        function showToast(msg) {
            const t = document.getElementById('toast');
            t.textContent = msg;
            t.classList.add('show');
            setTimeout(() => t.classList.remove('show'), 2500);
        }

        function shareDashboard(e) {
            e.preventDefault();
            // Try native share first (works on mobile + some desktops)
            if (navigator.share) {
                const blob = new Blob([document.documentElement.outerHTML], {type: 'text/html'});
                const file = new File([blob], 'ai_learning_dashboard.html', {type: 'text/html'});
                navigator.share({
                    title: 'AI Learning Dashboard',
                    text: 'Check out my AI learning journey â€” stories, pipeline, and key terms!',
                    files: [file]
                }).catch(() => fallbackShare());
            } else {
                fallbackShare();
            }
        }

        function fallbackShare() {
            // Download the HTML file so they can share it
            const blob = new Blob([document.documentElement.outerHTML], {type: 'text/html'});
            const url = URL.createObjectURL(blob);
            const a = document.createElement('a');
            a.href = url;
            a.download = 'ai_learning_dashboard.html';
            a.click();
            URL.revokeObjectURL(url);
            showToast('ğŸ“¥ Downloaded! Send this file to anyone â€” it works standalone.');
        }

        function filterTerms(query) {
            const rows = document.querySelectorAll('.term-row');
            const q = query.toLowerCase();
            rows.forEach(row => {
                const text = row.getAttribute('data-search');
                row.classList.toggle('hidden', q !== '' && !text.includes(q));
            });
        }
    </script>
</body>
</html>
