<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="robots" content="noindex, nofollow, noarchive">
    <meta name="referrer" content="no-referrer">
    <meta http-equiv="Permissions-Policy" content="camera=(), microphone=(), geolocation=()">
    <title>AI Learning â€” Ram's Journey</title>
    <style>
        * { margin: 0; padding: 0; box-sizing: border-box; }

        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
            background: #191919;
            color: #e0e0e0;
            height: 100vh;
            overflow: hidden;
        }

        /* Layout */
        .layout {
            display: flex;
            height: 100vh;
        }

        /* Sidebar */
        .sidebar {
            width: 260px;
            min-width: 260px;
            background: #202020;
            border-right: 1px solid #2d2d2d;
            padding: 20px 12px;
            overflow-y: auto;
            display: flex;
            flex-direction: column;
        }
        .sidebar-brand {
            padding: 8px 12px 20px;
            font-size: 14px;
            font-weight: 700;
            color: #9ca3af;
            letter-spacing: -0.3px;
        }
        .sidebar-brand span { color: #e0e0e0; }
        .sidebar-link {
            display: block;
            padding: 6px 12px;
            margin: 1px 0;
            color: #9ca3af;
            text-decoration: none;
            font-size: 14px;
            border-radius: 6px;
            transition: all 0.15s;
            line-height: 1.5;
        }
        .sidebar-link:hover {
            background: #2d2d2d;
            color: #e0e0e0;
        }
        .sidebar-link.active {
            background: #2d2d2d;
            color: #ffffff;
        }
        .sidebar-section-label {
            padding: 12px 12px 4px;
            font-size: 11px;
            font-weight: 700;
            color: #525252;
            letter-spacing: 1px;
        }
        .sidebar-divider {
            height: 1px;
            background: #2d2d2d;
            margin: 8px 12px;
        }
        .sidebar-footer {
            margin-top: auto;
            padding: 16px 12px 8px;
            font-size: 11px;
            color: #525252;
        }

        /* Content */
        .content {
            flex: 1;
            overflow-y: auto;
            padding: 40px 60px;
        }

        /* Pages (sections) */
        .page {
            display: none;
            max-width: 720px;
            animation: fadeIn 0.2s;
        }
        .page.active { display: block; }
        @keyframes fadeIn { from { opacity: 0; } to { opacity: 1; } }

        .back-link {
            display: inline-block;
            font-size: 13px;
            color: #8b5cf6;
            text-decoration: none;
            margin-bottom: 16px;
            padding: 4px 0;
            transition: color 0.15s;
        }
        .back-link:hover {
            color: #a78bfa;
        }

        .page-breadcrumb {
            font-size: 12px;
            color: #525252;
            margin-bottom: 8px;
            letter-spacing: 0.5px;
        }
        .page-title {
            font-size: 32px;
            font-weight: 700;
            color: #ffffff;
            margin-bottom: 6px;
            letter-spacing: -0.5px;
        }
        .page-meta {
            font-size: 14px;
            color: #6b7280;
            margin-bottom: 36px;
        }

        /* Content Blocks */
        .content-block {
            margin-bottom: 32px;
        }
        .block-label {
            font-size: 12px;
            font-weight: 700;
            letter-spacing: 0.5px;
            color: #6b7280;
            margin-bottom: 16px;
            padding-bottom: 8px;
            border-bottom: 1px solid #2d2d2d;
        }

        /* Kid Story */
        .kid-block {
            padding: 24px;
            background: #1c1917;
            border-radius: 8px;
            border-left: 3px solid #f59e0b;
        }
        .story-para {
            font-size: 15px;
            line-height: 1.85;
            color: #fef3c7;
            margin-bottom: 14px;
            font-family: Georgia, 'Times New Roman', serif;
        }
        .story-para:last-child { margin-bottom: 0; }

        /* Tech Story */
        .tech-block {
            padding: 24px;
            background: #1a1a2e;
            border-radius: 8px;
            border-left: 3px solid #8b5cf6;
        }
        .tech-para {
            font-size: 14px;
            line-height: 1.75;
            color: #c4b5fd;
            margin-bottom: 12px;
        }

        /* Share Story Button */
        .share-story-btn {
            display: inline-block;
            margin-top: 24px;
            padding: 10px 20px;
            background: #2d1b69;
            color: #c4b5fd;
            border: 1px solid #8b5cf6;
            border-radius: 8px;
            font-size: 14px;
            font-weight: 600;
            text-decoration: none;
            cursor: pointer;
            transition: all 0.15s;
        }
        .share-story-btn:hover {
            background: #3b2480;
            color: #e0d4ff;
            box-shadow: 0 2px 12px rgba(139, 92, 246, 0.25);
        }

        /* Diagram */
        .diagram-box {
            background: #0f0f1a;
            border-radius: 6px;
            padding: 16px;
            margin-top: 16px;
            overflow-x: auto;
        }
        .diagram-box pre {
            font-family: 'SF Mono', 'Fira Code', Menlo, Consolas, monospace;
            font-size: 13px;
            line-height: 1.5;
            color: #a78bfa;
            white-space: pre;
        }

        /* Week Tiles */
        .week-grid {
            display: grid;
            grid-template-columns: repeat(auto-fill, minmax(300px, 1fr));
            gap: 16px;
            margin-bottom: 32px;
        }
        .week-tile {
            background: #202020;
            border: 1px solid #2d2d2d;
            border-radius: 12px;
            padding: 20px;
            transition: border-color 0.2s, box-shadow 0.2s;
        }
        .week-tile:hover {
            border-color: var(--accent);
            box-shadow: 0 0 20px rgba(139, 92, 246, 0.08);
        }
        .week-tile.week-coming {
            opacity: 0.5;
        }
        .week-header {
            display: flex;
            justify-content: space-between;
            align-items: center;
            margin-bottom: 8px;
        }
        .week-number {
            font-size: 11px;
            font-weight: 700;
            letter-spacing: 1.2px;
            color: var(--accent);
        }
        .week-status {
            font-size: 11px;
            font-weight: 600;
            padding: 2px 8px;
            border-radius: 4px;
            background: rgba(139, 92, 246, 0.1);
            color: var(--accent);
        }
        .week-title {
            font-size: 18px;
            font-weight: 700;
            color: #e0e0e0;
            margin-bottom: 12px;
        }
        .week-progress-bar {
            height: 4px;
            background: #2d2d2d;
            border-radius: 2px;
            overflow: hidden;
            margin-bottom: 6px;
        }
        .week-progress-fill {
            height: 100%;
            border-radius: 2px;
            transition: width 0.3s;
        }
        .week-progress-text {
            font-size: 12px;
            color: #525252;
            margin-bottom: 14px;
        }
        .week-topics {
            display: flex;
            flex-direction: column;
            gap: 4px;
        }
        .week-topic {
            display: block;
            font-size: 13px;
            color: #9ca3af;
            padding: 6px 10px;
            border-radius: 6px;
            cursor: pointer;
            text-decoration: none;
            transition: background 0.15s, color 0.15s;
        }
        .week-topic:hover {
            background: #2d2d2d;
            color: #e0e0e0;
        }
        .week-topic.locked {
            color: #3d3d3d;
            cursor: default;
        }
        .week-topic.locked:hover {
            background: transparent;
            color: #3d3d3d;
        }

        /* Week Mastered â€” Gold Trophy */
        .week-tile.week-mastered {
            background: linear-gradient(135deg, #1a1508 0%, #2d2006 50%, #1a1508 100%);
            border-color: #d4a017;
            box-shadow: 0 0 20px rgba(212, 160, 23, 0.15);
            opacity: 1;
        }
        .week-tile.week-mastered .week-status {
            background: rgba(212, 160, 23, 0.2);
            color: #f5d251;
            font-weight: 700;
        }
        .week-tile.week-mastered .week-number {
            color: #f5d251;
        }
        .week-exam-info {
            font-size: 12px;
            color: #d4a017;
            margin-bottom: 10px;
            font-style: italic;
        }

        /* Week Review Required */
        .week-tile.week-review .week-status {
            background: rgba(245, 158, 11, 0.15);
            color: #f59e0b;
        }

        /* Week Unlocked â€” Pulse glow */
        .week-tile.week-unlocked {
            opacity: 1;
            border-color: #3b82f6;
            animation: unlockPulse 2s ease-in-out 3;
        }
        .week-tile.week-unlocked .week-status {
            background: rgba(59, 130, 246, 0.2);
            color: #60a5fa;
            font-weight: 700;
        }
        @keyframes unlockPulse {
            0%, 100% { box-shadow: 0 0 5px rgba(59, 130, 246, 0.1); }
            50% { box-shadow: 0 0 25px rgba(59, 130, 246, 0.3); }
        }

        /* Section Divider */
        .section-divider {
            text-align: center;
            margin: 8px 0 24px;
            position: relative;
        }
        .section-divider::before {
            content: '';
            position: absolute;
            top: 50%;
            left: 0;
            right: 0;
            height: 1px;
            background: #2d2d2d;
        }
        .section-divider span {
            position: relative;
            background: #191919;
            padding: 0 16px;
            font-size: 11px;
            font-weight: 700;
            letter-spacing: 1.2px;
            color: #525252;
        }

        /* Pipeline Steps */
        .pipeline-steps {
            display: flex;
            flex-direction: column;
            gap: 0;
        }
        .pipe-step {
            display: flex;
            align-items: center;
            gap: 16px;
            padding: 16px 20px;
            background: #202020;
            border: 1px solid #2d2d2d;
            border-radius: 12px;
            transition: transform 0.15s, box-shadow 0.15s;
        }
        .pipe-step {
            cursor: pointer;
        }
        .pipe-step:hover {
            transform: translateX(4px);
            box-shadow: 0 2px 12px rgba(139, 92, 246, 0.15);
            border-color: #3d3d3d;
        }
        .pipe-step.final {
            background: linear-gradient(135deg, #1a1a2e 0%, #2d1a4e 100%);
            border-color: #7c3aed;
        }
        .pipe-icon {
            width: 44px;
            height: 44px;
            min-width: 44px;
            border-radius: 10px;
            display: flex;
            align-items: center;
            justify-content: center;
            font-size: 20px;
        }
        .pipe-content {
            flex: 1;
        }
        .pipe-label {
            font-size: 10px;
            font-weight: 700;
            letter-spacing: 1.2px;
            color: #525252;
            margin-bottom: 2px;
        }
        .pipe-title {
            font-size: 15px;
            font-weight: 600;
            color: #e0e0e0;
            margin-bottom: 2px;
        }
        .pipe-desc {
            font-size: 13px;
            color: #6b7280;
            line-height: 1.4;
        }
        .pipe-output {
            font-size: 12px;
            font-weight: 600;
            color: #a78bfa;
            white-space: nowrap;
            padding: 4px 10px;
            background: #1a1a2e;
            border-radius: 6px;
            border: 1px solid #2d2d4e;
        }
        .pipe-connector {
            width: 2px;
            height: 12px;
            background: #3d3d3d;
            margin: 0 0 0 37px;
        }

        /* Key Terms */
        .search-input {
            width: 100%;
            padding: 10px 14px;
            background: #202020;
            border: 1px solid #2d2d2d;
            border-radius: 6px;
            color: #e0e0e0;
            font-size: 14px;
            outline: none;
            margin-bottom: 20px;
        }
        .search-input:focus { border-color: #8b5cf6; }
        .search-input::placeholder { color: #525252; }

        .term-row {
            padding: 12px 0;
            border-bottom: 1px solid #2d2d2d;
            display: grid;
            grid-template-columns: 180px 1fr;
            gap: 12px;
            align-items: baseline;
        }
        .term-row.hidden { display: none; }
        .term-name {
            font-size: 14px;
            font-weight: 600;
            color: #a78bfa;
        }
        .term-def {
            font-size: 14px;
            color: #9ca3af;
            line-height: 1.5;
        }
        .term-source {
            display: none;
        }

        /* Script */
        .script-box {
            padding: 28px;
            background: #1a1a2e;
            border-radius: 8px;
            border-left: 3px solid #8b5cf6;
        }
        .script-para {
            font-size: 16px;
            line-height: 1.9;
            color: #e0e7ff;
            margin-bottom: 16px;
            font-family: Georgia, 'Times New Roman', serif;
        }
        .script-para:last-child { margin-bottom: 0; }

        /* Share Toast */
        .toast {
            position: fixed;
            bottom: 24px;
            right: 24px;
            background: #7c3aed;
            color: #fff;
            padding: 12px 20px;
            border-radius: 8px;
            font-size: 14px;
            font-weight: 500;
            opacity: 0;
            transform: translateY(10px);
            transition: all 0.3s;
            z-index: 1000;
            pointer-events: none;
        }
        .toast.show {
            opacity: 1;
            transform: translateY(0);
        }

        /* Pixel Art Reveal */
        .pixel-grid {
            display: grid;
            grid-template-columns: repeat(13, 1fr);
            gap: 3px;
            max-width: 480px;
            margin: 24px auto;
            padding: 16px;
            background: #141414;
            border-radius: 12px;
            border: 1px solid #2a2a2a;
        }

        .pixel-cell {
            aspect-ratio: 1;
            border-radius: 3px;
            background: #222;
            transition: transform 0.2s, box-shadow 0.2s;
        }

        .pixel-cell.revealed {
            background: var(--pixel-color);
            box-shadow: 0 0 10px color-mix(in srgb, var(--pixel-color) 40%, transparent);
            cursor: pointer;
        }

        .pixel-cell.revealed:hover {
            transform: scale(1.3);
            box-shadow: 0 0 20px color-mix(in srgb, var(--pixel-color) 60%, transparent);
            z-index: 2;
        }

        /* Progress */
        .pixel-progress {
            max-width: 480px;
            margin: 0 auto 8px;
        }
        .pixel-progress-bar {
            height: 6px;
            background: #222;
            border-radius: 3px;
            overflow: hidden;
        }
        .pixel-progress-fill {
            height: 100%;
            background: linear-gradient(90deg, #a855f7, #3b82f6, #22d3ee);
            border-radius: 3px;
            transition: width 0.3s;
        }
        .pixel-progress-text {
            font-size: 12px;
            color: #525252;
            margin-top: 6px;
            text-align: center;
        }

        .pixel-footer {
            text-align: center;
            margin-top: 24px;
            color: #525252;
            font-size: 13px;
            font-style: italic;
        }

        /* Responsive */
        @media (max-width: 768px) {
            .sidebar { display: none; }
            .content { padding: 20px; }
            .page-title { font-size: 24px; }
            .term-row { grid-template-columns: 1fr; }
        }
    </style>
</head>
<body>
    <div class="layout">

        <!-- Sidebar -->
        <div class="sidebar">
            <div class="sidebar-brand">ğŸ§  <span>AI Learning</span></div>

            <a href="#pipeline" class="sidebar-link" onclick="showSection('pipeline')">ğŸ”— The Full Pipeline</a>
<a href="#lego" class="sidebar-link" onclick="showSection('lego')">ğŸ§± The Build</a>
<div class="sidebar-divider"></div>
<div class="sidebar-section-label">STORIES</div>
<a href="#topic-0" class="sidebar-link" onclick="showSection('topic-0')">1. 4-Step LLM Learning Path</a>
<a href="#topic-1" class="sidebar-link" onclick="showSection('topic-1')">2. How LLMs Are Built + Data Prep</a>
<a href="#topic-2" class="sidebar-link" onclick="showSection('topic-2')">3. Tokenization</a>
<a href="#topic-3" class="sidebar-link" onclick="showSection('topic-3')">4. Transformers</a>
<a href="#topic-4" class="sidebar-link" onclick="showSection('topic-4')">5. Model Architecture</a>
<a href="#topic-5" class="sidebar-link" onclick="showSection('topic-5')">6. Model Training</a>
<a href="#topic-6" class="sidebar-link" onclick="showSection('topic-6')">7. Text Generation</a>
<a href="#topic-7" class="sidebar-link" onclick="showSection('topic-7')">8. Post Training SFT</a>
<a href="#topic-8" class="sidebar-link" onclick="showSection('topic-8')">9. Reinforcement Learning</a>
<a href="#topic-9" class="sidebar-link" onclick="showSection('topic-9')">10. Evaluation of LLMs</a>
<div class="sidebar-divider"></div>
<a href="#terms" class="sidebar-link" onclick="showSection('terms')">ğŸ“š Key Terms</a>
<a href="#script" class="sidebar-link" onclick="showSection('script')">ğŸ¤ 2-Min Script</a>


            <div class="sidebar-divider"></div>
            <a href="#" class="sidebar-link share-btn" onclick="shareDashboard(event)">ğŸ“¤ Share This Page</a>

            <div class="sidebar-footer">
                Updated Feb 26, 2026<br>
                10 topics â€¢ 67 terms
            </div>
        </div>

        <!-- Content -->
        <div class="content">

            <!-- Pipeline -->
            <div class="page active" id="pipeline">
                <div class="page-breadcrumb">Overview</div>
                <h1 class="page-title">The Full Pipeline</h1>
                <div class="page-meta">How every LLM is built â€” from raw internet to helpful chatbot</div>

                <!-- Week Tiles -->
                <div class="week-grid">
                    
            <div class="week-tile week-review" style="--accent: #8b5cf6;">
                <div class="week-header">
                    <div class="week-number">WEEK 1</div>
                    <div class="week-status">ğŸ“‹ Review Required</div>
                </div>
                <div class="week-title">How LLMs Are Built</div>
                
                <div class="week-progress-bar">
                    <div class="week-progress-fill" style="width: 100%; background: #8b5cf6;"></div>
                </div>
                <div class="week-progress-text">10/10 topics</div>
                <div class="week-topics">
                    <a class="week-topic" onclick="navigateTo('topic-0')">4-Step LLM Learning Path</a><a class="week-topic" onclick="navigateTo('topic-1')">How LLMs Are Built + Data Prep</a><a class="week-topic" onclick="navigateTo('topic-2')">Tokenization</a><a class="week-topic" onclick="navigateTo('topic-3')">Transformers</a><a class="week-topic" onclick="navigateTo('topic-4')">Model Architecture</a><a class="week-topic" onclick="navigateTo('topic-5')">Model Training</a><a class="week-topic" onclick="navigateTo('topic-6')">Text Generation</a><a class="week-topic" onclick="navigateTo('topic-7')">Post Training SFT</a><a class="week-topic" onclick="navigateTo('topic-8')">Reinforcement Learning</a><a class="week-topic" onclick="navigateTo('topic-9')">Evaluation of LLMs</a>
                </div>
            </div>
        
            <div class="week-tile week-coming" style="--accent: #3b82f6;">
                <div class="week-header">
                    <div class="week-number">WEEK 2</div>
                    <div class="week-status">Coming Soon</div>
                </div>
                <div class="week-title">RAG & Retrieval</div>
                
                <div class="week-progress-bar">
                    <div class="week-progress-fill" style="width: 0%; background: #3b82f6;"></div>
                </div>
                <div class="week-progress-text">0/1 topics</div>
                <div class="week-topics">
                    <span class="week-topic locked">Topics coming soon...</span>
                </div>
            </div>
        
                </div>

                <div class="section-divider">
                    <span>THE 6-STEP PIPELINE</span>
                </div>

                <div class="pipeline-steps">
                    
                    <div class="pipe-step" onclick="navigateTo('topic-1')" >
                        <div class="pipe-icon" style="background: #3b82f6;">ğŸŒ</div>
                        <div class="pipe-content">
                            <div class="pipe-label">STEP 1</div>
                            <div class="pipe-title">Crawl the Internet</div>
                            <div class="pipe-desc">2.7B pages â€” Common Crawl vacuums the web</div>
                        </div>
                        <div class="pipe-output">200-400 TB raw</div>
                    </div>
                    <div class="pipe-connector"></div>
        
                    <div class="pipe-step" onclick="navigateTo('topic-1')" >
                        <div class="pipe-icon" style="background: #10b981;">ğŸ§¹</div>
                        <div class="pipe-content">
                            <div class="pipe-label">STEP 2</div>
                            <div class="pipe-title">Clean the Data</div>
                            <div class="pipe-desc">Remove HTML, duplicates, PII, toxic content</div>
                        </div>
                        <div class="pipe-output">44 TB clean (FineWeb)</div>
                    </div>
                    <div class="pipe-connector"></div>
        
                    <div class="pipe-step" onclick="navigateTo('topic-2')" >
                        <div class="pipe-icon" style="background: #f59e0b;">ğŸ”¢</div>
                        <div class="pipe-content">
                            <div class="pipe-label">STEP 3</div>
                            <div class="pipe-title">Tokenize</div>
                            <div class="pipe-desc">Text â†’ numbers using BPE (50K-200K vocab)</div>
                        </div>
                        <div class="pipe-output">15T tokens</div>
                    </div>
                    <div class="pipe-connector"></div>
        
                    <div class="pipe-step" onclick="navigateTo('topic-4')" >
                        <div class="pipe-icon" style="background: #8b5cf6;">ğŸ“</div>
                        <div class="pipe-content">
                            <div class="pipe-label">STEP 4</div>
                            <div class="pipe-title">Embed â†’ Transform â†’ Softmax</div>
                            <div class="pipe-desc">Token IDs â†’ vectors â†’ transformer blocks â†’ probabilities â†’ next token</div>
                        </div>
                        <div class="pipe-output">The architecture</div>
                    </div>
                    <div class="pipe-connector"></div>
        
                    <div class="pipe-step" onclick="navigateTo('topic-5')" >
                        <div class="pipe-icon" style="background: #ef4444;">ğŸ”„</div>
                        <div class="pipe-content">
                            <div class="pipe-label">STEP 5</div>
                            <div class="pipe-title">Pre-Train</div>
                            <div class="pipe-desc">Forward â†’ Loss â†’ Backward â†’ Repeat 1M+ times</div>
                        </div>
                        <div class="pipe-output">$10M-$100M+ ğŸ’¸</div>
                    </div>
                    <div class="pipe-connector"></div>
        
                    <div class="pipe-step" onclick="navigateTo('topic-7')" >
                        <div class="pipe-icon" style="background: #ec4899;">ğŸ“</div>
                        <div class="pipe-content">
                            <div class="pipe-label">STEP 6</div>
                            <div class="pipe-title">Post-Train (SFT + RL)</div>
                            <div class="pipe-desc">14K-100K Q&A pairs â†’ genius learns manners</div>
                        </div>
                        <div class="pipe-output">Helpful + Safe</div>
                    </div>
                    <div class="pipe-connector"></div>
        
                    <div class="pipe-step final">
                        <div class="pipe-icon" style="background: #7c3aed;">ğŸ¤–</div>
                        <div class="pipe-content">
                            <div class="pipe-label">RESULT</div>
                            <div class="pipe-title">ChatGPT / Claude / Llama</div>
                            <div class="pipe-desc">Same recipe â€” some just use bigger LEGO towers</div>
                        </div>
                        <div class="pipe-output">Ship it! ğŸš€</div>
                    </div>
    
                </div>
            </div>

            <!-- LEGO Camera Build -->
            
        <div class="page" id="lego">
            <div class="page-breadcrumb">Visual Map</div>
            <h1 class="page-title">The Build</h1>
            <div class="page-meta">10 / 90 pixels revealed</div>

            <div class="pixel-progress">
                <div class="pixel-progress-bar">
                    <div class="pixel-progress-fill" style="width: 11%;"></div>
                </div>
                <div class="pixel-progress-text">11% complete â€” 80 to go</div>
            </div>

            <div class="pixel-grid">
                <div class="pixel-cell"></div>
<div class="pixel-cell revealed" style="--pixel-color: #a855f7;" onclick="navigateTo('topic-4')" title="Model Architecture"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell revealed" style="--pixel-color: #22d3ee;" onclick="navigateTo('topic-7')" title="Post Training SFT"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell revealed" style="--pixel-color: #22d3ee;" onclick="navigateTo('topic-5')" title="Model Training"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell revealed" style="--pixel-color: #a855f7;" onclick="navigateTo('topic-6')" title="Text Generation"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell revealed" style="--pixel-color: #22d3ee;" onclick="navigateTo('topic-0')" title="4-Step LLM Learning Path"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell revealed" style="--pixel-color: #22d3ee;" onclick="navigateTo('topic-9')" title="Evaluation of LLMs"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell revealed" style="--pixel-color: #a855f7;" onclick="navigateTo('topic-3')" title="Transformers"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell revealed" style="--pixel-color: #a855f7;" onclick="navigateTo('topic-1')" title="How LLMs Are Built + Data Prep"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell revealed" style="--pixel-color: #22d3ee;" onclick="navigateTo('topic-8')" title="Reinforcement Learning"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell revealed" style="--pixel-color: #a855f7;" onclick="navigateTo('topic-2')" title="Tokenization"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>
<div class="pixel-cell"></div>

            </div>

            <div class="pixel-footer">
                <p>Every topic reveals a pixel. What's the image? Keep learning to find out.</p>
            </div>
        </div>
    

            <!-- Story Pages -->
            
        <div class="page" id="topic-0">
            <a class="back-link" href="#" onclick="navigateTo('pipeline'); return false;">â† Back to Pipeline</a>
            <div class="page-breadcrumb">Chapter 1 of 10</div>
            <h1 class="page-title">4-Step LLM Learning Path</h1>
            <div class="page-meta">Starting point â€” the roadmap</div>

            <div class="content-block kid-block">
                <div class="block-label">ğŸ“– Explain It Like I'm 10</div>
                <p class="story-para">Imagine you want to build a house. Would you start by putting up the roof? No way! You'd start with the foundation, then walls, then the roof, and finally the furniture inside. ğŸ—ï¸</p>
<p class="story-para">Learning AI works EXACTLY the same way. There are 4 steps, and you MUST do them in order:</p>
<p class="story-para">Step 1 is learning the basics â€” what is a neural network? How does a machine learn from data? This is your foundation. Skip this, and everything else feels like magic you can't explain. ğŸ§±</p>
<p class="story-para">Step 2 is learning about Transformers â€” the special brain design from 2017 that changed everything. It's like learning how the engine works before you try to build a car. ğŸï¸</p>
<p class="story-para">Step 3 is the fun part â€” pre-training (teaching the brain from scratch, costs $100 MILLION!), fine-tuning (teaching it YOUR specific job, much cheaper), and RAG (giving it a cheat sheet to look things up without retraining). ğŸ“š</p>
<p class="story-para">Step 4 is building real things â€” AI agents that can use tools, browse the web, and actually DO stuff. This is the roof and furniture â€” it only works because you built everything underneath first! ğŸ¤–</p>
<p class="story-para">The secret? All the resources for these 4 steps are FREE. Andrew Ng, Jay Alammar, Hugging Face â€” they all teach this for free online!</p>

            </div>

            <div class="content-block tech-block">
                <div class="block-label">ğŸ¬ 60-Second Technical Version</div>
                <p class="tech-para">Here's the thing most people get wrong when learning AI â€” they jump straight to the flashy stuff. They want to build AI agents and chatbots on day one, without understanding how any of it works underneath. It's like trying to customize a car engine when you've never learned what a piston does. The 4-step learning path exists because each step is the foundation for the next, and skipping ahead means everything feels like unexplainable magic.</p>
<p class="tech-para">Step 1 is ML fundamentals â€” neural networks, gradient descent, how a machine actually 'learns' from data. This is the part most people skip because it feels academic. But without it, you can't debug anything. When your model isn't working, you won't know if the problem is in your data, your architecture, or your training loop. Andrew Ng's free courses cover this in about 2 weeks, and it pays dividends forever.</p>
<p class="tech-para">Step 2 is Transformers â€” the architecture that changed everything in 2017. Before Transformers, AI models read text one word at a time (RNNs) and forgot the beginning of a sentence by the time they reached the end. Transformers read everything at once using a mechanism called 'attention.' Jay Alammar's 'Illustrated Transformer' blog is considered the single best resource for understanding this â€” it turns dense math into visual intuition.</p>
<p class="tech-para">Step 3 is where the money is â€” literally. Pre-training means teaching a model from scratch on the entire internet, and it costs $10M to $100M+. Fine-tuning takes that pre-trained model and specializes it for your specific task, for a fraction of the cost. And RAG (Retrieval-Augmented Generation) is the clever shortcut â€” instead of retraining the model, you give it a search engine to look things up in real time. Most production AI systems today use RAG because retraining is too expensive and too slow.</p>
<p class="tech-para">Step 4 is applications â€” AI agents that can browse the web, call APIs, write code, and actually DO things in the real world. This is where the industry is headed in 2025-2026, but it only works when you understand everything underneath. An agent that calls a tool is just a Transformer generating a special token that triggers a function call â€” if you didn't learn Steps 1-3, that sentence means nothing to you.</p>
<p class="tech-para">The most surprising part? Every resource for all 4 steps is free. Andrew Ng (Coursera), Jay Alammar (blog), Hugging Face (courses and models), and dozens of open-source projects. The barrier to becoming an AI engineer isn't money â€” it's discipline. You have to resist the temptation to skip ahead.</p>

                <div class="diagram-box">
                    <pre>  THE 4-STEP AI ENGINEER PATH
  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  Step 1: ML FUNDAMENTALS           ğŸ§±
  â”‚  Neural networks, gradient descent
  â”‚  Skip this = can't debug anything
  â”‚  Resources: Andrew Ng (free)
  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  Step 2: TRANSFORMERS              ğŸï¸
  â”‚  Attention mechanism (2017)
  â”‚  How ALL modern LLMs work
  â”‚  Resources: Jay Alammar (free)
  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  Step 3: PRE-TRAIN / FINE-TUNE / RAG
  â”‚  Pre-train: $10M-$100M+ (from scratch)
  â”‚  Fine-tune: $cheap (specialize it)
  â”‚  RAG: $free (give it a search engine)
  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  Step 4: AI AGENTS &amp; APPS          ğŸ¤–
  â”‚  Tools, APIs, real-world actions
  â”‚  Only works WITH Steps 1-3
  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  âš ï¸  SKIP A STEP = HOUSE FALLS DOWN
  âœ…  ALL resources are FREE
  ğŸ¯  Barrier isn't money â€” it's discipline</pre>
                </div>
            </div>

            <a class="share-story-btn" href="#" onclick="shareStory(0); return false;">ğŸ“§ Share This Story</a>
        </div>
        
        <div class="page" id="topic-1">
            <a class="back-link" href="#" onclick="navigateTo('pipeline'); return false;">â† Back to Pipeline</a>
            <div class="page-breadcrumb">Chapter 2 of 10</div>
            <h1 class="page-title">How LLMs Are Built + Data Prep</h1>
            <div class="page-meta">Topic 1 â€” zoom into Step 3 of the roadmap</div>

            <div class="content-block kid-block">
                <div class="block-label">ğŸ“– Explain It Like I'm 10</div>
                <p class="story-para">Imagine you want to teach a robot to speak every language in the world. Where do you start? The INTERNET! ğŸŒ</p>
<p class="story-para">First, you send out a web crawler â€” a little spider bot that visits 2.7 BILLION web pages and copies everything it finds. That's 200-400 terabytes of raw messy text. Think of it like vacuuming the entire internet into a giant bag. ğŸ•·ï¸</p>
<p class="story-para">But that bag is FILTHY â€” it's got HTML code, duplicate pages, people's private info, and toxic content. So you run it through a cleaning machine. Common Crawl collects the raw data, and teams like Hugging Face clean it into something called FineWeb â€” 44 terabytes of squeaky clean text. ğŸ§¹</p>
<p class="story-para">Now you feed that clean text into the model and let it train. This is the EXPENSIVE part â€” GPT-3 cost $10 million, GPT-4 cost over $100 million! Thousands of GPUs running for months. ğŸ’¸</p>
<p class="story-para">But here's the funny part â€” after all that training, the model is like a genius who never learned manners. It knows EVERYTHING but can't hold a conversation! So you do post-training â€” teach it to be helpful and safe. That's much cheaper and faster. ğŸ“</p>
<p class="story-para">And THAT'S how every LLM is built â€” vacuum the internet, clean it, train on it, then teach it manners!</p>

            </div>

            <div class="content-block tech-block">
                <div class="block-label">ğŸ¬ 60-Second Technical Version</div>
                <p class="tech-para">Most people think building an LLM starts with some brilliant algorithm. It doesn't. It starts with a vacuum cleaner. Specifically, a web crawler that visits 2.7 billion web pages and downloads everything it finds â€” every Wikipedia article, every Reddit thread, every blog post, every news story. The result is a staggering 200 to 400 terabytes of raw text. An organization called Common Crawl has been doing this since 2008, and their dataset is freely available to anyone. This is the raw material that every major LLM is built from.</p>
<p class="tech-para">But here's what most people don't realize â€” that raw data is absolutely filthy. It's full of HTML tags, JavaScript code, duplicate pages (the same article copied across 50 websites), people's personal information (names, emails, phone numbers), and toxic content. You can't just feed this into a model. Data cleaning is roughly 90% of the work in building an LLM, and it's the least glamorous part. Teams like Hugging Face spent months building FineWeb â€” a cleaned version of Common Crawl that went from 200+ TB of mess down to 44 terabytes of high-quality text, roughly 15 trillion tokens. The cleaning pipeline includes deduplication, PII removal, language filtering, quality scoring, and toxic content filtering.</p>
<p class="tech-para">Once the data is clean, it gets tokenized â€” converted from human-readable text into numbers that the model can process. Then comes pre-training: you feed those trillions of tokens into the model architecture and let it learn patterns. This is where the enormous cost comes in. GPT-3 cost roughly $10 million to train. GPT-4 is estimated at over $100 million. You're running thousands of specialized GPUs (like NVIDIA's H100s) for weeks or months straight, burning through electricity and compute.</p>
<p class="tech-para">After pre-training, you have something remarkable but also useless for conversation â€” a 'base model.' It can finish any sentence brilliantly because it's seen the entire internet, but ask it 'What's the capital of France?' and instead of answering, it might generate ten more questions (because on the internet, questions are often followed by more questions, not answers). It's a genius who never learned manners.</p>
<p class="tech-para">That's why every LLM goes through post-training â€” first Supervised Fine-Tuning (SFT) to teach it the format of helpful conversations, then Reinforcement Learning (RL) to make it safe and honest. The entire pipeline from crawling to deployment can take 6-12 months and cost tens of millions of dollars. And yet, the resulting model â€” the thing that seems like magic when you chat with it â€” is just statistics learned from the internet.</p>

                <div class="diagram-box">
                    <pre>  HOW EVERY LLM IS BUILT
  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  ğŸŒ THE INTERNET (all human knowledge)
       â”‚
  â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ CRAWL (Common Crawl)            â”‚
  â”‚ 2.7 billion pages               â”‚
  â”‚ Result: 200-400 TB raw text     â”‚
  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ CLEAN (90% of the work!)        â”‚
  â”‚ Remove: HTML, duplicates, PII   â”‚
  â”‚ Remove: toxic content, low-qual â”‚
  â”‚ Result: 44 TB clean (FineWeb)   â”‚
  â”‚         = 15 trillion tokens    â”‚
  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ PRE-TRAIN (the expensive part)  â”‚
  â”‚ Thousands of GPUs, weeks/months â”‚
  â”‚ Cost: $10M (GPT-3) â†’ $100M+    â”‚
  â”‚ Result: genius with no manners  â”‚
  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ POST-TRAIN (teach it manners)   â”‚
  â”‚ SFT â†’ helpful, RL â†’ safe       â”‚
  â”‚ Much cheaper + faster           â”‚
  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â–¼
  ğŸ¤– Ready to chat!
  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  ğŸ“Š SCALE: 200TB raw â†’ 44TB clean â†’ 1 model
  ğŸ’¡ Data cleaning = 90% of the work</pre>
                </div>
            </div>

            <a class="share-story-btn" href="#" onclick="shareStory(1); return false;">ğŸ“§ Share This Story</a>
        </div>
        
        <div class="page" id="topic-2">
            <a class="back-link" href="#" onclick="navigateTo('pipeline'); return false;">â† Back to Pipeline</a>
            <div class="page-breadcrumb">Chapter 3 of 10</div>
            <h1 class="page-title">Tokenization</h1>
            <div class="page-meta">Topic 2 â€” data is clean, now convert text â†’ numbers</div>

            <div class="content-block kid-block">
                <div class="block-label">ğŸ“– Explain It Like I'm 10</div>
                <p class="story-para">Here's a problem: computers don't understand words. They ONLY understand numbers. So how do you get a computer to read "Tell me a joke"? You need a translator! That translator is called a Tokenizer. ğŸ”¢</p>
<p class="story-para">The tokenizer has a dictionary â€” a vocabulary â€” with every word-piece it knows and a number for each one. "Tell" = 41, "me" = 553, "a" = 264, "joke" = 22691. Now the computer can work with it!</p>
<p class="story-para">But how big should the dictionary be? If you use WHOLE WORDS, your dictionary needs 500,000+ entries (every word on the internet!). That's way too big. If you use SINGLE LETTERS, the dictionary is tiny (just 26 letters!) but "Hello" becomes 5 tokens instead of 1 â€” everything takes forever. âŒ</p>
<p class="story-para">The sweet spot? Sub-word tokenization! Break words into PIECES. "unhappiness" becomes "un" + "happiness". Common words stay whole, rare words get split up. Dictionary size: 50,000 to 200,000 entries. Perfect balance! âœ…</p>
<p class="story-para">The most popular way to build this dictionary is called BPE â€” Byte Pair Encoding. It starts with individual letters, then keeps merging the most popular pairs together. "h"+"u"+"g" â†’ "hu"+"g" â†’ "hug". Like building bigger LEGO pieces from smaller ones until you have the right set! ğŸ§±</p>
<p class="story-para">Fun fact: type gibberish like "xqzplm" and it explodes into tiny tokens. Type "the cat sat" and it's just 3 tokens. The tokenizer knows common words!</p>

            </div>

            <div class="content-block tech-block">
                <div class="block-label">ğŸ¬ 60-Second Technical Version</div>
                <p class="tech-para">Here's a fundamental problem that most people never think about â€” computers don't understand text. Not even a little bit. They only understand numbers. So before any AI model can process the sentence 'Tell me a joke,' that sentence needs to be converted into a sequence of numbers. That's what a tokenizer does, and the way it does it matters more than you'd expect.</p>
<p class="tech-para">The obvious approach is word-level tokenization â€” give every word its own number. 'Tell' = 41, 'me' = 553, 'joke' = 22691. Simple, right? The problem is vocabulary size. The English internet contains over 500,000 unique words (including misspellings, slang, technical terms, names). Your vocabulary table becomes enormous, and every new word the model encounters is 'unknown.' The opposite extreme â€” character-level tokenization â€” uses a tiny vocabulary (just 26 letters plus punctuation), but now 'Hello' is 5 tokens instead of 1. Sequences become 6.5x longer, which means 6.5x more computation, 6.5x more memory, and the model struggles to learn word-level meaning from individual characters.</p>
<p class="tech-para">The breakthrough was sub-word tokenization â€” specifically an algorithm called BPE (Byte Pair Encoding). BPE starts with individual characters and repeatedly merges the most frequent pair into a new token. First pass: 'h' and 'e' appear together constantly, so merge them into 'he.' Next pass: 'he' and 'l' merge into 'hel.' Keep going until you reach your target vocabulary size â€” typically 50,000 to 200,000 tokens. The result is elegant: common words like 'the' stay as single tokens, while rare words like 'unhappiness' get split into 'un' + 'happiness.' The model gets the efficiency of word-level for common text and the flexibility of character-level for rare words.</p>
<p class="tech-para">One thing that surprises people is that every model has its OWN tokenizer with its OWN vocabulary. GPT-3.5 uses roughly 100,000 tokens. GPT-4o expanded to about 200,000 tokens. The same sentence produces completely different token IDs across different models â€” 'Hello' might be token 15339 in GPT-3 and token 9906 in Llama. This is why you can't just swap models in a pipeline without also swapping the tokenizer.</p>
<p class="tech-para">You can actually see this in action â€” type a common phrase like 'the cat sat' and it's just 3 tokens. Type gibberish like 'xqzplm' and it explodes into 5-6 tiny tokens because the tokenizer has never seen that character combination before. The tokenizer is the unsung hero of the entire LLM pipeline â€” invisible when it works, catastrophic when it doesn't.</p>

                <div class="diagram-box">
                    <pre>  HOW TEXT BECOMES NUMBERS
  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  Input: "Tell me a joke"
       â”‚
  â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ STEP 1: SPLIT into sub-words    â”‚
  â”‚ [Tell] [me] [a] [joke]          â”‚
  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ STEP 2: LOOKUP in vocabulary    â”‚
  â”‚ Tellâ†’41, meâ†’553, aâ†’264         â”‚
  â”‚ jokeâ†’22691                      â”‚
  â”‚ Output: [41, 553, 264, 22691]   â”‚
  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â–¼
  Numbers the model can process! ğŸ½ï¸
  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  WHY SUB-WORDS? (THE 3 OPTIONS)
  âŒ Word-level  â†’ vocab 500K+ (too big)
     Every new word = "unknown"
  âŒ Char-level  â†’ vocab 26 (too small)
     "Hello" = 5 tokens, 6.5x slower
  âœ… Sub-word (BPE) â†’ vocab 50K-200K â­
     Common words stay whole
     Rare words get split smartly
  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  BPE ALGORITHM (how vocab is built):
  Start: [h] [u] [g]  (individual chars)
  Merge: [hu] [g]     (most frequent pair)
  Merge: [hug]         (repeat until done)
  ğŸ¯ Target: 50K-200K tokens</pre>
                </div>
            </div>

            <a class="share-story-btn" href="#" onclick="shareStory(2); return false;">ğŸ“§ Share This Story</a>
        </div>
        
        <div class="page" id="topic-3">
            <a class="back-link" href="#" onclick="navigateTo('pipeline'); return false;">â† Back to Pipeline</a>
            <div class="page-breadcrumb">Chapter 4 of 10</div>
            <h1 class="page-title">Transformers</h1>
            <div class="page-meta">Topic 3 â€” tokens exist, now what brain design processes them?</div>

            <div class="content-block kid-block">
                <div class="block-label">ğŸ“– Explain It Like I'm 10</div>
                <p class="story-para">In 2017, a team at Google published a paper called "Attention Is All You Need" and changed EVERYTHING. Before this, AI read text one word at a time, like reading through a tiny keyhole â€” slow and forgetful. ğŸ”‘</p>
<p class="story-para">The Transformer reads ALL words at once! And it has a superpower called "attention" â€” it figures out which words are important to each other. Read this: "The cat sat on the mat because IT was tired." What does "it" mean? The cat or the mat? YOU know it's the cat. The Transformer figures this out too â€” by letting every word "look at" every other word and decide what matters. That's attention! ğŸ‘ï¸</p>
<p class="story-para">The original Transformer had two halves: an Encoder (reads and understands text) and a Decoder (generates new text). It was built for translation â€” French in, English out. ğŸ‡«ğŸ‡·â¡ï¸ğŸ‡¬ğŸ‡§</p>
<p class="story-para">But then researchers made a wild discovery: throw away the encoder, keep ONLY the decoder, and you get something that's AMAZING at generating text. One word at a time, predicting what comes next. This is called a "decoder-only" transformer. ğŸ”®</p>
<p class="story-para">And here's the punchline â€” ChatGPT, Claude, Llama, Gemini â€” EVERY modern AI chatbot uses this exact same decoder-only design. The 2017 paper literally changed the world! ğŸŒ</p>

            </div>

            <div class="content-block tech-block">
                <div class="block-label">ğŸ¬ 60-Second Technical Version</div>
                <p class="tech-para">Before 2017, AI language models had a serious problem â€” they read text one word at a time, in order, like looking through a keyhole. These were called RNNs (Recurrent Neural Networks), and they had a fatal flaw: by the time they reached word 50 in a sentence, they'd largely forgotten word 1. Imagine reading a novel but only remembering the last paragraph. Long documents, complex references, subtle context â€” all lost. Researchers tried patches (LSTMs, GRUs), but the fundamental bottleneck remained: sequential processing meant you couldn't parallelize training, and the 'memory' degraded over distance.</p>
<p class="tech-para">In 2017, a team at Google published a paper titled 'Attention Is All You Need' â€” arguably the most important AI paper ever written. Their key insight was radical: throw away the sequential processing entirely. Instead, let every word in a sentence look at every OTHER word simultaneously. This mechanism is called 'self-attention.' Consider the sentence: 'The cat sat on the mat because IT was tired.' What does 'it' refer to? The cat or the mat? Humans resolve this instantly. Self-attention does the same thing â€” 'it' computes a relevance score with every other word, discovers that 'cat' has the highest score, and strengthens that connection. This happens for ALL words at ALL positions at the same time. That's the magic.</p>
<p class="tech-para">The original Transformer architecture had two halves â€” an Encoder (which reads and understands the input) and a Decoder (which generates the output). It was designed for machine translation: feed French into the encoder, get English out of the decoder. But researchers quickly discovered something unexpected. If you throw away the encoder and keep only the decoder, you get a model that's extraordinarily good at generating text â€” one token at a time, predicting what comes next based on everything before it. This 'decoder-only' architecture became the foundation of a revolution.</p>
<p class="tech-para">GPT-1 (2018) proved the concept with 117 million parameters. GPT-2 (2019) scaled to 1.5 billion. GPT-3 (2020) exploded to 175 billion â€” and suddenly, the model could write essays, code, and poetry. The architecture hadn't changed. The math was identical. They just made it bigger and fed it more data. That's the shocking simplicity of the Transformer â€” it's the same building block, stacked and scaled.</p>
<p class="tech-para">Today, every major AI model you interact with â€” ChatGPT, Claude, Llama, Gemini, Mistral â€” is a decoder-only Transformer. The 2017 paper didn't just introduce a new technique; it obsoleted an entire generation of AI architectures. Jay Alammar's 'Illustrated Transformer' blog post remains the best visual explanation of how attention works under the hood, and it's free to read.</p>

                <div class="diagram-box">
                    <pre>  THE TRANSFORMER REVOLUTION
  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  BEFORE (RNNs): Read one word at a time
  â”Œâ”€â”â†’â”Œâ”€â”â†’â”Œâ”€â”â†’â”Œâ”€â”â†’â”Œâ”€â”â†’ ... â†’ â”Œâ”€â”
  â”‚w1â”‚ â”‚w2â”‚ â”‚w3â”‚ â”‚w4â”‚ â”‚w5â”‚       â”‚w50â”‚
  â””â”€â”˜ â””â”€â”˜ â””â”€â”˜ â””â”€â”˜ â””â”€â”˜       â””â”€â”˜
  âŒ By word 50, word 1 is forgotten
  âŒ Sequential = can't parallelize
  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  AFTER (2017): "Attention Is All You Need"
  Every word looks at EVERY other word:
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  w1 â†â†’ w2 â†â†’ w3 â†â†’ ... wN â”‚
  â”‚  ALL connected at once!     â”‚
  â”‚  "it" â†’ strongest link to   â”‚
  â”‚         "cat" (not "mat")   â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  âœ… Parallel processing
  âœ… Long-range context preserved
  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  THE SPLIT:
  Original: [ENCODER|DECODER] â†’ translation
  Modern:   [DECODER ONLY]    â†’ generation
  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  GPT, Claude, Llama, Gemini = ALL
  decoder-only Transformers â˜ï¸</pre>
                </div>
            </div>

            <a class="share-story-btn" href="#" onclick="shareStory(3); return false;">ğŸ“§ Share This Story</a>
        </div>
        
        <div class="page" id="topic-4">
            <a class="back-link" href="#" onclick="navigateTo('pipeline'); return false;">â† Back to Pipeline</a>
            <div class="page-breadcrumb">Chapter 5 of 10</div>
            <h1 class="page-title">Model Architecture</h1>
            <div class="page-meta">Topics 3 + 4 â€” combine tokens + transformers into the full pipeline</div>

            <div class="content-block kid-block">
                <div class="block-label">ğŸ“– Explain It Like I'm 10</div>
                <p class="story-para">Imagine you're playing a word-guessing game with a really smart robot. You say "I hope you are" and the robot has to guess what comes next. ğŸ¤–</p>
<p class="story-para">But here's the thing â€” the robot can't read words! It only understands numbers. So first, a helper called the Tokenizer chops your words into secret number codes. "I" becomes 42, "hope" becomes 891, and so on.</p>
<p class="story-para">But plain numbers aren't enough. The robot needs RICHER numbers â€” like a whole list of clues for each word. So the Embedding Layer turns each number code into a long list of clues. Think of it like turning a name tag into a full personality profile! ğŸ·ï¸â¡ï¸ğŸ“‹</p>
<p class="story-para">Now comes the big brain part. Those clue-lists pass through a bunch of Transformer Blocks â€” like a stack of really smart filters. Each filter makes the clues better and better. GPT-2 has 12 filters. GPT-3 has 96. Same type of filter, just MORE of them â€” like building a taller LEGO tower! ğŸ§±</p>
<p class="story-para">Finally, the robot looks at the LAST clue-list and asks: "Out of all 50,000 words I know, which one should come next?" It gives every word a score. "well" gets 86 points. "happy" gets 11. "banana" gets 0. Highest score wins! ğŸ†</p>
<p class="story-para">And that's it â€” that's how ChatGPT, Claude, and every AI chatbot works. Same recipe, just some use bigger LEGO towers than others!</p>

            </div>

            <div class="content-block tech-block">
                <div class="block-label">ğŸ¬ 60-Second Technical Version</div>
                <p class="tech-para">If you open the hood of any modern AI chatbot â€” ChatGPT, Claude, Llama, Gemini â€” you'll find the exact same 4-stage pipeline. Not similar. Identical in structure. The only difference between a small model and a massive one is the size of each stage. Understanding this pipeline is like understanding the assembly line in a factory â€” once you see it, every model makes sense.</p>
<p class="tech-para">Stage 1 is the Tokenizer, which chops text into token IDs (covered in the tokenization topic). Stage 2 is where it gets interesting â€” the Embedding Layer. This takes each token ID (just a plain number like 42) and converts it into a dense vector â€” a list of hundreds or thousands of decimal numbers. Think of it as turning a name tag into a full personality profile. Token 42 ('I') becomes something like [0.2, -0.8, 0.5, 1.1, ...] â€” a 768-dimensional vector in GPT-2, or a 12,288-dimensional vector in GPT-3. These embedding vectors aren't hardcoded; they're learned during training. The model discovers that similar words should have similar vectors â€” 'king' and 'queen' end up near each other in this high-dimensional space.</p>
<p class="tech-para">Stage 3 is the Transformer Blocks â€” the actual 'brain' of the model. Each block contains the self-attention mechanism (from the Transformer paper) plus a feed-forward neural network. GPT-2 has 12 blocks stacked on top of each other. GPT-3 has 96. Llama 3 has 126. Same type of block, just more of them â€” like building a taller LEGO tower. Every block refines the vectors further, adding more context and understanding. By the time the vectors exit the final block, they've been enriched with deep contextual meaning.</p>
<p class="tech-para">Stage 4 is the head â€” Linear projection followed by Softmax. Here's the critical detail most people miss: only the LAST token's vector matters. The model takes that single vector, projects it across the entire vocabulary (roughly 50,000 words), and Softmax converts those raw scores into probabilities that sum to 1.0. 'Well' might get 0.86 (86%), 'happy' gets 0.11, 'banana' gets 0.0001. The highest probability wins â€” or more precisely, a sampling algorithm picks from the top candidates.</p>
<p class="tech-para">What makes this truly elegant is that GPT-2 (1.5 billion parameters), GPT-3 (175 billion), and Llama 3 (405 billion) all use this EXACT same 4-stage pipeline. The only things that change are the hyperparameters â€” number of layers, vector dimension, vocabulary size. The architecture is identical. It's like the difference between a 2-story house and a 96-story skyscraper â€” same blueprint, just scaled up.</p>

                <div class="diagram-box">
                    <pre>  THE 4-STAGE LLM PIPELINE
  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  Input: "I hope you are ___"
       â”‚
  â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ STAGE 1: TOKENIZER              â”‚
  â”‚ Text â†’ IDs: [42, 891, 67, 55]  â”‚
  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ STAGE 2: EMBEDDING LAYER        â”‚
  â”‚ IDs â†’ vectors (learned!)        â”‚
  â”‚ 42 â†’ [0.2, -0.8, 0.5, ...]    â”‚
  â”‚ Like: name tag â†’ personality    â”‚
  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ STAGE 3: TRANSFORMER BLOCKS     â”‚
  â”‚ Attention + Feed-Forward         â”‚
  â”‚ GPT-2: 12 blocks (1.5B params)  â”‚
  â”‚ GPT-3: 96 blocks (175B params)  â”‚
  â”‚ Llama: 126 blocks (405B params) â”‚
  â”‚ Same block type, just MORE      â”‚
  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚ âš¡ Only LAST vector survives
  â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ STAGE 4: LINEAR + SOFTMAX       â”‚
  â”‚ Last vector â†’ 50K probabilities â”‚
  â”‚ "well": 86%  "happy": 11%      â”‚
  â”‚ "banana": 0.01%  â†’ pick one!   â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  ğŸ”‘ Same pipeline for ALL models
     Only the SIZE changes</pre>
                </div>
            </div>

            <a class="share-story-btn" href="#" onclick="shareStory(4); return false;">ğŸ“§ Share This Story</a>
        </div>
        
        <div class="page" id="topic-5">
            <a class="back-link" href="#" onclick="navigateTo('pipeline'); return false;">â† Back to Pipeline</a>
            <div class="page-breadcrumb">Chapter 6 of 10</div>
            <h1 class="page-title">Model Training</h1>
            <div class="page-meta">Topic 5 â€” architecture is set, now how does it LEARN?</div>

            <div class="content-block kid-block">
                <div class="block-label">ğŸ“– Explain It Like I'm 10</div>
                <p class="story-para">Imagine you build a robot that finishes your sentences. You say "peanut butter and..." and it should say "jelly!" But when you first turn it on, it says "elephant!" because its brain is full of random nonsense. ğŸ¤–</p>
<p class="story-para">So here's how you teach it. You grab a sentence from a book â€” "The cat sat on the..." â€” and you KNOW the answer is "mat." You let the robot guess. It says "pizza!" with 15% confidence. Then you use a GRADE MACHINE (that's the loss function) â€” it compares "pizza" with "mat" and gives a grade: 3.0. In this game, high is BAD â€” like golf! â›³</p>
<p class="story-para">Then a HELPER ROBOT (that's the optimizer) looks at the grade, goes into the robot's brain, and turns billions of tiny knobs so that "mat" gets a little more likely and "pizza" gets a little less likely next time. ğŸ”§</p>
<p class="story-para">One round doesn't fix it. But you do this a MILLION times with different sentences. Slowly, the robot stops saying "elephant" and starts saying "jelly." Not because anyone TOLD it peanut butter goes with jelly â€” it figured it out from seeing the pattern thousands of times! That's implicit knowledge. ğŸ§ </p>
<p class="story-para">The code for this? About 10 lines. But running it on 405 billion knobs? That needs 16,000 of the world's best computers running for weeks, costing millions of dollars. Simple recipe, enormous kitchen! ğŸ—ï¸</p>

            </div>

            <div class="content-block tech-block">
                <div class="block-label">ğŸ¬ 60-Second Technical Version</div>
                <p class="tech-para">Here's what surprises most people about training â€” the algorithm is shockingly simple. It's a 2-step loop. Step 1 (the forward pass): feed a sentence into the model with the last word hidden, let the model guess what that word should be, and compute a single number called the 'loss' using cross-entropy. Loss is like a golf score â€” high is bad. If the correct answer is 'physicist' and the model guesses it with only 5% confidence, the loss is high. If the model guesses it with 90% confidence, the loss is nearly zero.</p>
<p class="tech-para">Step 2 (the backward pass): the optimizer takes that loss number and works backward through every single layer of the model, computing exactly how much each weight contributed to the error. Then it nudges each weight by a tiny amount in the direction that would reduce the loss. This is called gradient descent â€” literally descending the slope of the error landscape. The model has billions of these weights (405 billion in Llama 3), and every single one gets adjusted, every single round.</p>
<p class="tech-para">Repeat this loop a million times with different sentences from the internet, and something remarkable happens. The model starts discovering patterns that nobody explicitly taught it. It learns that 'Einstein' connects to 'physicist,' that 'machine learning' connects to 'algorithms,' that 'peanut butter' connects to 'jelly' â€” not because anyone programmed these associations, but because the statistical patterns in the training data made these connections reduce the loss. This is called 'implicit knowledge,' and it's why LLMs can seem to 'understand' things â€” they don't understand in the human sense, they've internalized statistical relationships across the entire internet.</p>
<p class="tech-para">The code for this training loop is about 10 lines of Python. The math is undergraduate calculus. There's nothing exotic about it. What IS exotic is the scale. Training Llama 3 (405 billion parameters) required 16,000 NVIDIA H100 GPUs, over 7 terabytes of GPU memory, and cost millions of dollars in compute. Meta's training cluster consumed enough electricity to power a small town. The training ran for weeks, processing trillions of tokens.</p>
<p class="tech-para">One critical detail that trips people up: training happens ONCE. It's not an ongoing process. You gather your data, you run the training loop for weeks, and then you stop. The model's weights are frozen. When you chat with ChatGPT, it's not learning from your conversation â€” it's running inference on frozen weights. The next version (GPT-5, Claude 4) will be a completely separate training run from scratch, with new data and new compute. Training is a one-time investment that produces a static artifact.</p>

                <div class="diagram-box">
                    <pre>  HOW AN LLM LEARNS
  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  Input: "Einstein was a German-born ___"
  Correct answer: "physicist"
       â”‚
  â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ STEP 1: FORWARD PASS            â”‚
  â”‚ Feed text â†’ model guesses       â”‚
  â”‚ "physicist" confidence: 5%      â”‚
  â”‚ Loss = 3.0 (high = bad!) â›³     â”‚
  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ STEP 2: BACKWARD PASS           â”‚
  â”‚ Optimizer traces error backward â”‚
  â”‚ Nudges EVERY weight (billions!) â”‚
  â”‚ "physicist": 5% â†’ 6% (better)  â”‚
  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
  ğŸ”„ REPEAT 1,000,000+ TIMES
       â”‚
       â–¼
  ğŸ§  IMPLICIT KNOWLEDGE EMERGES
  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  THE SCALE:
  Code: ~10 lines of Python
  Llama 3: 405B weights, 16K H100 GPUs
  Cost: millions of $$$, weeks of runtime
  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  âš ï¸ Training happens ONCE, then stops
  ChatGPT isn't learning from your chat â€”
  it's running frozen weights</pre>
                </div>
            </div>

            <a class="share-story-btn" href="#" onclick="shareStory(5); return false;">ğŸ“§ Share This Story</a>
        </div>
        
        <div class="page" id="topic-6">
            <a class="back-link" href="#" onclick="navigateTo('pipeline'); return false;">â† Back to Pipeline</a>
            <div class="page-breadcrumb">Chapter 7 of 10</div>
            <h1 class="page-title">Text Generation</h1>
            <div class="page-meta">Topic 6 â€” model is trained, now how does it PRODUCE text?</div>

            <div class="content-block kid-block">
                <div class="block-label">ğŸ“– Explain It Like I'm 10</div>
                <p class="story-para">Imagine you have a super smart robot that knows every word in every book ever written. You ask it to finish your sentence: "Once upon a..." But the robot doesn't just KNOW the answer â€” it has a list of 50,000 possible next words, each with a score of how likely it is. ğŸ¤–</p>
<p class="story-para">So how does it pick? The lazy way (Greedy Search) is to always pick the word with the highest score. "time" wins at 40%! Easy! But here's the problem â€” the robot gets stuck in a loop. Like a kid who tells the same joke over and over: "and then... and then... and then..." It picks the same popular words FOREVER. ğŸ”„</p>
<p class="story-para">So smart scientists said: "What if we let the robot be a LITTLE random?" That's Top-P Sampling. Instead of picking just the #1 word, you grab all the top words until their scores add up to 90%. If the robot is SURE the next word is "time" (89% confident), it only looks at that one word. But if it's unsure â€” maybe "time" is 30%, "a" is 25%, "the" is 20% â€” it grabs ALL of those and randomly picks from them. The robot adapts! Confident = focused. Unsure = explore! ğŸ¯</p>
<p class="story-para">And there's a temperature knob. Turn it DOWN for homework (be precise!). Turn it UP for storytelling (be wild!). That's why when you ask ChatGPT to write code, it's careful and exact. But when you ask it to write a poem, it surprises you every time. ğŸŒ¡ï¸</p>
<p class="story-para">The whole thing is just a for loop: pick a word, add it to the sentence, pick the next word, repeat. Stop when the robot says "THE END" or you hit a word limit. That's it â€” that's how every AI writes! âœ¨</p>

            </div>

            <div class="content-block tech-block">
                <div class="block-label">ğŸ¬ 60-Second Technical Version</div>
                <p class="tech-para">Here's something most people don't realize â€” after all that expensive training ($100M+, months of GPU time), the model doesn't actually 'know' what to say. It knows probabilities. You type 'Albert' and the model runs a forward pass through its billions of parameters and outputs a probability distribution over its entire vocabulary â€” roughly 50,000 possible next tokens. 'Einstein' might get 42%, 'John' gets 15%, 'was' gets 8%, and so on. The question is: how do you pick from that list?</p>
<p class="tech-para">The naive approach is Greedy Search â€” always pick the highest probability token. Simple, fast, but fatally flawed. It causes repetitive loops because the model keeps choosing the same 'safe' high-probability words. OpenAI demonstrated this with GPT-2: ask it about a cute dog and it literally gets stuck repeating the same sentence. Beam Search improved on this by tracking K parallel paths (like exploring K branches of a decision tree simultaneously), but it's still deterministic â€” same input always produces the same output, which makes the text feel robotic.</p>
<p class="tech-para">The real breakthrough was moving from deterministic to stochastic (random) sampling. Top-K Sampling picks randomly from the K most likely tokens â€” but K is fixed, which is a problem. Sometimes the model is very confident (one token has 90% probability) and you want to focus. Other times it's unsure and you want to explore. Top-P (Nucleus) Sampling solved this elegantly: instead of a fixed K, you keep adding tokens from highest to lowest probability until their cumulative probability exceeds a threshold P (usually 0.9). This means K is DYNAMIC â€” when confident, only 1-2 tokens qualify. When uncertain, maybe 20 tokens qualify. The model naturally adapts its creativity based on context.</p>
<p class="tech-para">Finally, Temperature acts as a creativity dial on TOP of the sampling algorithm. Low temperature (0.1-0.3) sharpens the probability distribution â€” the rich get richer, making output precise and focused (ideal for code generation). High temperature (0.7-1.0) flattens it â€” giving underdog tokens a fighting chance, making output more creative and surprising (ideal for storytelling, poetry). Every time you adjust that 'temperature' slider in ChatGPT or Claude, this is exactly what's happening.</p>
<p class="tech-para">The entire generation process is surprisingly simple in code â€” it's a for loop. Feed in the prompt, get probabilities, sample one token, append it to the input, repeat. Stop when the model outputs a special &lt;EOS&gt; (End of Sequence) token, or when you hit a maximum length. That's it. Every AI response you've ever read was generated exactly this way â€” one token at a time, in a loop.</p>

                <div class="diagram-box">
                    <pre>  HOW EVERY AI RESPONSE IS GENERATED
  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  Input: "Albert"        (your prompt)
       â–¼
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚   LLM   â”‚â”€â”€â”€â–¶â”‚ 50K probs    â”‚â”€â”€â”€â–¶â”‚ SAMPLING â”‚
  â”‚(forward  â”‚    â”‚ Einstein:42% â”‚    â”‚ALGORITHM â”‚
  â”‚  pass)   â”‚    â”‚ John: 15%    â”‚    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚ was: 8%      â”‚         â”‚
       â–²         â”‚ the: 5%      â”‚         â–¼
       â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   "Einstein"
       â”‚                                  â”‚
       â””â”€â”€â”€â”€â”€ append to input â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       ğŸ”„ REPEAT until &lt;EOS&gt; or max length
  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  THE EVOLUTION OF SAMPLING:
  âŒ Greedy    â†’ always pick #1 â†’ repetition loops
  âŒ Beam(K=5) â†’ track 5 paths  â†’ still deterministic
  âš ï¸ Top-K     â†’ random from top K â†’ but K is fixed
  âœ… Top-P â­  â†’ dynamic K based on confidence!
     Confident (90% on one token) â†’ K=1, focused
     Uncertain (spread across 20)  â†’ K=20, explore
  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  ğŸŒ¡ï¸ TEMPERATURE (the creativity dial):
     Low (0.1)  â†’ sharp probs â†’ precise (code)
     High (0.9) â†’ flat probs  â†’ creative (poetry)</pre>
                </div>
            </div>

            <a class="share-story-btn" href="#" onclick="shareStory(6); return false;">ğŸ“§ Share This Story</a>
        </div>
        
        <div class="page" id="topic-7">
            <a class="back-link" href="#" onclick="navigateTo('pipeline'); return false;">â† Back to Pipeline</a>
            <div class="page-breadcrumb">Chapter 8 of 10</div>
            <h1 class="page-title">Post Training SFT</h1>
            <div class="page-meta">Topic 7 â€” model generates text, now make it actually useful</div>

            <div class="content-block kid-block">
                <div class="block-label">ğŸ“– Explain It Like I'm 10</div>
                <p class="story-para">Remember that genius who never learned manners? The one who read the entire internet but can't hold a conversation? That's a base model after pre-training. It can finish any sentence, but ask it "What's the capital of France?" and it might just keep rambling instead of answering. ğŸ¤·</p>
<p class="story-para">SFT â€” Supervised Fine-Tuning â€” is like sending that genius to finishing school. You give it 14,000 to 100,000 examples of perfect conversations: "Question: What's 2+2? Answer: 4." Over and over, until it learns the FORMAT of being helpful. ğŸ“</p>
<p class="story-para">Here's the wild part â€” the training algorithm is IDENTICAL to pre-training! Same math, same optimizer, same loop. The ONLY thing that changes is the DATA. Instead of messy internet text, you feed it carefully curated question-answer pairs written by experts. Quality over quantity! âœ¨</p>
<p class="story-para">After SFT, the model can answer questions. But it's still not safe â€” it might help you do dangerous things if you ask nicely. That's where step 2 comes in: Reinforcement Learning (RL) teaches it to be safe and honest. ğŸ›¡ï¸</p>
<p class="story-para">Think of it like Pega: Pre-training = installing the platform. SFT = configuring it for your business. RL = user testing to catch edge cases. Same platform, just refined step by step!</p>

            </div>

            <div class="content-block tech-block">
                <div class="block-label">ğŸ¬ 60-Second Technical Version</div>
                <p class="tech-para">After pre-training, you have a model that's absorbed the entire internet â€” but it's essentially useless as an assistant. Ask a base model 'What's the capital of France?' and instead of answering 'Paris,' it might generate 'What's the capital of Germany? What's the capital of Spain?' â€” because on the internet, lists of questions are common, so generating more questions is a statistically reasonable next-token prediction. The base model is a text completion engine, not a conversation partner. It has all the knowledge but none of the behavior.</p>
<p class="tech-para">Supervised Fine-Tuning (SFT) fixes this by showing the model thousands of examples of the exact behavior you want. You create a dataset of instruction-response pairs: 'Summarize this article' â†’ [perfect summary]. 'Write a Python function that...' â†’ [clean code]. 'Explain quantum physics simply' â†’ [clear explanation]. InstructGPT used 14,500 of these pairs. Stanford's Alpaca used 52,000. Dolly used 15,000. The surprising discovery was that you don't need millions â€” a few thousand high-quality examples are enough to fundamentally shift the model's behavior.</p>
<p class="tech-para">Here's the part that blows people's minds â€” the training algorithm for SFT is IDENTICAL to pre-training. Same forward pass, same cross-entropy loss function, same optimizer, same backward pass. Nothing changes in the math. The ONLY difference is the data format. Pre-training uses raw internet text (trillions of tokens, months of compute, $100M+). SFT uses curated Q&amp;A pairs (thousands of examples, hours of compute, much cheaper). Same algorithm, radically different data, completely different behavior. This is one of the most elegant insights in modern AI â€” you don't need a new algorithm to teach new behavior, you just need the right examples.</p>
<p class="tech-para">But SFT alone isn't enough. An SFT-trained model will follow instructions, but it has no concept of safety. Ask it to write malware, and it'll try its best â€” because the SFT data taught it to be helpful, not to be cautious. That's where Reinforcement Learning from Human Feedback (RLHF) comes in. Human raters rank multiple model responses from best to worst, a 'reward model' learns those preferences, and then the LLM is fine-tuned to maximize the reward signal. This teaches the model to refuse harmful requests, avoid hallucination, and be honest about uncertainty.</p>
<p class="tech-para">The full post-training pipeline is: Base Model â†’ SFT (learns to follow instructions) â†’ RLHF (learns to be safe and honest) â†’ Production Model. Think of it in Pega terms: pre-training is installing the platform, SFT is configuring it for your business process, and RLHF is UAT testing to catch edge cases. Same platform at every stage â€” just progressively refined with better data and feedback.</p>

                <div class="diagram-box">
                    <pre>  FROM GENIUS TO HELPFUL ASSISTANT
  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  BASE MODEL (after pre-training)
  â”‚ Knows everything, helps with nothing
  â”‚ "What's the capital of France?"
  â”‚  â†’ "What's the capital of Germany?" âŒ
  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  â”‚              â–¼
  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  â”‚ STEP 1: SFT                  â”‚
  â”‚  â”‚ 14K-100K instruction pairs   â”‚
  â”‚  â”‚ Same algorithm as pre-train! â”‚
  â”‚  â”‚ Only the DATA changes        â”‚
  â”‚  â”‚ Hours (not months), cheap    â”‚
  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â”‚             â–¼
  â”‚  Model follows instructions âœ…
  â”‚  But no safety guardrails âš ï¸
  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  â”‚              â–¼
  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  â”‚ STEP 2: RLHF                 â”‚
  â”‚  â”‚ Humans rank responses        â”‚
  â”‚  â”‚ Reward model learns prefs    â”‚
  â”‚  â”‚ LLM optimizes for reward     â”‚
  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â”‚             â–¼
  â”‚  Model is safe + honest âœ…ğŸ›¡ï¸
  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  ğŸ“Š Pre-train: trillions tokens, $100M
     SFT: thousands examples, $cheap
     RLHF: human feedback, $moderate
  ğŸ¯ Same algorithm â€” different data!</pre>
                </div>
            </div>

            <a class="share-story-btn" href="#" onclick="shareStory(7); return false;">ğŸ“§ Share This Story</a>
        </div>
        
        <div class="page" id="topic-8">
            <a class="back-link" href="#" onclick="navigateTo('pipeline'); return false;">â† Back to Pipeline</a>
            <div class="page-breadcrumb">Chapter 9 of 10</div>
            <h1 class="page-title">Reinforcement Learning</h1>
            <div class="page-meta">Topic 8 â€” SFT model answers questions, now make it GOOD at answering</div>

            <div class="content-block kid-block">
                <div class="block-label">ğŸ“– Explain It Like I'm 10</div>
                <p class="story-para">Imagine you taught a robot to answer homework questions (that was SFT). It answers, but it's lazy â€” ask "how do I learn math?" and it says "read a book." Technically correct, but not helpful! ğŸ˜¤</p>
<p class="story-para">So you give the robot a practice drill. You ask it the same question 7 times and it gives 7 different answers. Some are great ("start with addition, then multiplication, then practice with fun games!") and some are terrible ("just give up" ğŸ˜…). You mark the good ones with âœ… and bad ones with âŒ, then tell the robot: "Make more answers like the green ones!"</p>
<p class="story-para">But here's the tricky part â€” for math homework, you can just CHECK if 2+2=4. Easy! âœ… That's a verifiable task. But what about "write me a poem"? There's no right answer! ğŸ¤” So you hire a bunch of teachers to RANK the poems: "this one's best, this one's okay, this one stinks." Then you build a robot teacher (the reward model) that learns from those rankings. Now the robot teacher can score poems WITHOUT needing humans every time!</p>
<p class="story-para">The secret sauce? An algorithm called PPO that takes all those scores and adjusts the robot's brain so it writes better answers next time. Practice â†’ score â†’ adjust â†’ repeat. That's reinforcement learning â€” turning a lazy C-student robot into an A-student! ğŸ†</p>

            </div>

            <div class="content-block tech-block">
                <div class="block-label">ğŸ¬ 60-Second Technical Version</div>
                <p class="tech-para">Here's something most people don't realize: after supervised fine-tuning, your LLM can answer questions â€” but it gives you the equivalent of a one-sentence text message when you asked for directions. "Take Andrew Ng's course." Thanks, but not helpful. Reinforcement learning exists because correctness isn't enough â€” you need quality.</p>
<p class="tech-para">The mechanism is elegant: it's a practice algorithm. You feed a prompt to the SFT model and let it generate seven different responses. Some are detailed and helpful, others are lazy or even harmful ("shame on you, try meditation" â€” good advice, terrible delivery). The trick is scoring them: which responses should the model produce MORE of?</p>
<p class="tech-para">This is where the world splits into two categories. Verifiable tasks â€” math, coding â€” have checkable answers. Is 2+2=4? Run the code, does it pass? A Python script labels each response correct or incorrect, and an algorithm called PPO (Proximal Policy Optimization) â€” or the newer GRPO â€” updates the model parameters to reinforce correct answers. Simple, automatic, done.</p>
<p class="tech-para">Unverifiable tasks â€” writing, brainstorming, naming your startup â€” have no right answer. So you need RLHF: Reinforcement Learning from Human Feedback, a two-step process. First, hire human annotators to rank responses: "Paris" beats "It's in Europe" beats "Eiffel Tower." Convert those rankings into winning/losing pairs. Train a separate reward model using margin ranking loss â€” a loss function that pushes the winning response's score UP and the losing response's score DOWN, maximizing the gap. Now this reward model is your proxy for humans â€” it scores any response automatically. Step two: same PPO loop. Model practices, reward model scores, PPO reinforces high-scoring responses.</p>
<p class="tech-para">The proof is in the demo: Llama 3.1 405B Base just rambles with more questions when asked "how to learn ML?" The post-trained version delivers a formatted, step-by-step guide with bold headers. Four stages â€” pre-training (trillions of tokens, months), SFT (10K-100K pairs, days), reward model (human-ranked comparisons, margin ranking loss), and RL (PPO reinforcement, days) â€” transform a trillion-parameter text completer into ChatGPT.</p>

                <div class="diagram-box">
                    <pre>â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  REINFORCEMENT LEARNING â€” The Practice Algorithm
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

  SFT Model ("I answer... barely")
       â”‚
       â–¼
  Generate 7 responses to same prompt
       â”‚
       â”œâ”€â”€ VERIFIABLE? (math/code)
       â”‚     â”‚
       â”‚     â–¼
       â”‚   Auto-check (Python/logic)
       â”‚     â”‚
       â”‚     â–¼
       â”‚   Labels: âœ… correct / âŒ wrong
       â”‚
       â”œâ”€â”€ UNVERIFIABLE? (writing/brainstorming)
       â”‚     â”‚
       â”‚     â–¼
       â”‚   RLHF Step 1: Train Reward Model
       â”‚     ğŸ‘¤ Humans rank â†’ win/lose pairs
       â”‚     ğŸ“ Margin ranking loss (maximize gap)
       â”‚     â”‚
       â”‚     â–¼
       â”‚   Reward Model scores responses (0.1-0.9)
       â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                   â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  PPO / GRPO          â”‚
        â”‚  Reinforce âœ… HIGH   â”‚
        â”‚  Discourage âŒ LOW   â”‚
        â”‚  Update SFT params   â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  ğŸ† FINAL MODEL      â”‚
        â”‚  Detailed, helpful,  â”‚
        â”‚  safe, polite        â”‚
        â”‚  = ChatGPT ğŸš€       â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  4 STAGES: Pre-train â†’ SFT â†’ Reward Model â†’ RL
  KEY TERMS: PPO, GRPO, RLHF, margin ranking loss
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•</pre>
                </div>
            </div>

            <a class="share-story-btn" href="#" onclick="shareStory(8); return false;">ğŸ“§ Share This Story</a>
        </div>
        
        <div class="page" id="topic-9">
            <a class="back-link" href="#" onclick="navigateTo('pipeline'); return false;">â† Back to Pipeline</a>
            <div class="page-breadcrumb">Chapter 10 of 10</div>
            <h1 class="page-title">Evaluation of LLMs</h1>
            <div class="page-meta">Topic 9 â€” model is trained, now how do we know if it's any good?</div>

            <div class="content-block kid-block">
                <div class="block-label">ğŸ“– Explain It Like I'm 10</div>
                <p class="story-para">Imagine your school is having a bake-off and you need to judge whose cookies are best! ğŸª</p>
<p class="story-para">First, you try judging in your kitchen BEFORE the bake sale â€” that's offline evaluation. You have three ways: You could count ingredients exactly (perplexity â€” "did they follow the recipe perfectly?") but that's boring because perfect-recipe cookies can still taste terrible! ğŸ˜ So instead you give them REAL tests â€” "make chocolate chip cookies, make brownies, make a cake" â€” and see how each one turns out. That's benchmarks like PIQA and HellaSwag. Or you could hire a fancy chef to taste everything, but chefs are biased â€” one loves chocolate, another hates it! ğŸ§‘â€ğŸ³</p>
<p class="story-para">Then comes the bake sale â€” cookies are OUT and real people are eating them. That's online evaluation! People give thumbs up ğŸ‘ or thumbs down ğŸ‘ while eating (that's human feedback in ChatGPT). But the COOLEST way? LM Arena â€” like a BLIND taste test! ğŸ­ Someone puts TWO cookies in front of you, you don't know who baked them, you pick your favorite, and THEN they reveal who made it! Berkeley students built this for AI, and 2.9 MILLION people have voted!</p>
<p class="story-para">Oh, and the bakery itself? There's a GUARD at the door checking if your ORDER is okay ğŸšª (no "make me poison cookies!"), then a HELPER who fixes your messy handwriting on the order ğŸ“, then the BAKER makes the cookies ğŸª, and ANOTHER guard checks the cookies BEFORE you eat them! Two guards â€” one checks what goes IN, one checks what comes OUT! ğŸ›¡ï¸ğŸ›¡ï¸</p>

            </div>

            <div class="content-block tech-block">
                <div class="block-label">ğŸ¬ 60-Second Technical Version</div>
                <p class="tech-para">Here's something most people get wrong about LLM evaluation: they think it's one thing. It's actually two completely different worlds â€” and confusing them is like confusing a lab test with a customer review.</p>
<p class="tech-para">Offline evaluation happens before anyone sees the model. The oldest method is perplexity â€” feed the model "How are you" and measure how confidently it predicts "doing" next (probability 0.93). Lower perplexity means better prediction. But here's why nobody trusts it anymore: a model that perfectly reproduces text can still give useless answers to real questions. Reproducing isn't the same as reasoning. So the industry moved to task-specific benchmarks â€” standardized tests across six domains: common-sense reasoning (PIQA, HellaSwag), world knowledge, reading comprehension, math (GSM8K), code generation (HumanEval), and composite benchmarks (MMLU). You run thousands of questions through the model, compare answers against the correct ones, and score it. You can also hire human experts, but they're expensive, slow, and biased â€” two experts will score the same response differently.</p>
<p class="tech-para">Online evaluation happens after deployment with real users. The simple version: ChatGPT's thumbs up/thumbs down button. Every click is a signal â€” used for evaluation AND fed back into RL training (dual purpose). But the gold standard is LM Arena (lmarena.ai), built by Berkeley graduates. It's a blind taste test for AI: you submit a prompt, two anonymous models respond, you vote for the better one, identities are revealed after. With 2.9 million votes across 211+ models, it produces ELO rankings like chess. Gemini 2.5 Pro leads at 1446, followed by o3 at 1435.</p>
<p class="tech-para">And evaluation is just one layer. The full system design reveals that the trained LLM is a tiny piece of the puzzle. Every prompt passes through an input safety filter (guardrails), a prompt enhancer (fixes typos, grammar, ambiguity using ML + heuristics), the response generator (Top-p sampling with the LLM), and then an output safety evaluator â€” a second set of guardrails checking the response before the user sees it. Two checkpoints, not one. Session management appends your full chat history to every new prompt â€” that's how ChatGPT "remembers" your conversation.</p>
<p class="tech-para">The punchline? Building the LLM takes months and millions of dollars. But the system around it â€” the guards, the enhancers, the memory â€” is what turns a statistical model into a product 2.9 million people vote on.</p>

                <div class="diagram-box">
                    <pre>â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
          LLM EVALUATION â€” THE FULL MAP
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ OFFLINE (Before Deploy) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚                                                â”‚
  â”‚  Traditional â”€â”€â–¶ Task-Specific â”€â”€â–¶ Human       â”‚
  â”‚  (Perplexity)    (PIQA,HellaSwag)  (Experts)   â”‚
  â”‚   âš ï¸ obsolete     âœ… standard       âš ï¸ biased  â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                   [ DEPLOY ]
                        â”‚
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ONLINE (After Deploy) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚                                                â”‚
  â”‚  Human Feedback â”€â”€â”€â”€â”€â”€â”€â”€â–¶ Crowdsourcing        â”‚
  â”‚  (ğŸ‘/ğŸ‘ in ChatGPT)       (LM Arena)           â”‚
  â”‚   feeds back to RL        blind vote, ELO      â”‚
  â”‚                           2.9M votes, 211+     â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
          SYSTEM DESIGN â€” 5 COMPONENTS
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

  Prompt â–¶ ğŸ›¡ï¸ Safety â–¶ ğŸ“ Enhance â–¶ ğŸ¤– Generate â–¶ ğŸ›¡ï¸ Safety â–¶ Response
           (input)     (fix typos)   (LLM+Top-p)   (output)
              â”‚                                        â”‚
              â””â”€â”€â”€â”€ unsafe? â”€â”€â–¶ ğŸš« REJECT â—€â”€â”€ unsafe? â”€â”˜

  + Session Management (appends chat history each time)

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  ğŸ† LM Arena: Blind taste test for AI
     Built by Berkeley grads | ELO scoring
     #1 Gemini 2.5 Pro (1446) | 2.9M votes
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•</pre>
                </div>
            </div>

            <a class="share-story-btn" href="#" onclick="shareStory(9); return false;">ğŸ“§ Share This Story</a>
        </div>
        

            <!-- Key Terms -->
            <div class="page" id="terms">
                <div class="page-breadcrumb">Reference</div>
                <h1 class="page-title">Key Terms</h1>
                <div class="page-meta">67 terms across all topics</div>

                <input type="text" class="search-input" placeholder="Search terms..." oninput="filterTerms(this.value)">
                
        <div class="term-row" data-search="neural network a math formula that learns patterns from data">
            <div class="term-name">Neural Network</div>
            <div class="term-def">A math formula that learns patterns from data</div>
            <div class="term-source">The 4-Step Learning Path</div>
        </div>
        
        <div class="term-row" data-search="transformer the brain architecture behind every modern ai (invented 2017)">
            <div class="term-name">Transformer</div>
            <div class="term-def">The brain architecture behind every modern AI (invented 2017)</div>
            <div class="term-source">The 4-Step Learning Path</div>
        </div>
        
        <div class="term-row" data-search="pre-training teaching ai from scratch using the internet ($100m+)">
            <div class="term-name">Pre-training</div>
            <div class="term-def">Teaching AI from scratch using the internet ($100M+)</div>
            <div class="term-source">The 4-Step Learning Path</div>
        </div>
        
        <div class="term-row" data-search="fine-tuning teaching it your specific job (much cheaper)">
            <div class="term-name">Fine-tuning</div>
            <div class="term-def">Teaching it YOUR specific job (much cheaper)</div>
            <div class="term-source">The 4-Step Learning Path</div>
        </div>
        
        <div class="term-row" data-search="rag giving the ai a reference book to look things up (no retraining needed)">
            <div class="term-name">RAG</div>
            <div class="term-def">Giving the AI a reference book to look things up (no retraining needed)</div>
            <div class="term-source">The 4-Step Learning Path</div>
        </div>
        
        <div class="term-row" data-search="common crawl free public dataset that crawls the internet every 1-2 months">
            <div class="term-name">Common Crawl</div>
            <div class="term-def">Free public dataset that crawls the internet every 1-2 months</div>
            <div class="term-source">How LLMs Are Built + Data Prep</div>
        </div>
        
        <div class="term-row" data-search="fineweb hugging face's cleaned dataset (44 tb, 15 trillion tokens)">
            <div class="term-name">FineWeb</div>
            <div class="term-def">Hugging Face's cleaned dataset (44 TB, 15 trillion tokens)</div>
            <div class="term-source">How LLMs Are Built + Data Prep</div>
        </div>
        
        <div class="term-row" data-search="pii removal stripping out personal info (names, emails, phone numbers)">
            <div class="term-name">PII removal</div>
            <div class="term-def">Stripping out personal info (names, emails, phone numbers)</div>
            <div class="term-source">How LLMs Are Built + Data Prep</div>
        </div>
        
        <div class="term-row" data-search="base model the "genius with no manners" after pre-training">
            <div class="term-name">Base model</div>
            <div class="term-def">The "genius with no manners" after pre-training</div>
            <div class="term-source">How LLMs Are Built + Data Prep</div>
        </div>
        
        <div class="term-row" data-search="post-training teaching the genius to be helpful and safe">
            <div class="term-name">Post-training</div>
            <div class="term-def">Teaching the genius to be helpful and safe</div>
            <div class="term-source">How LLMs Are Built + Data Prep</div>
        </div>
        
        <div class="term-row" data-search="tokenizer converts text â†” numbers">
            <div class="term-name">Tokenizer</div>
            <div class="term-def">Converts text â†” numbers</div>
            <div class="term-source">Tokenization</div>
        </div>
        
        <div class="term-row" data-search="vocabulary the dictionary of all known word-pieces and their ids">
            <div class="term-name">Vocabulary</div>
            <div class="term-def">The dictionary of all known word-pieces and their IDs</div>
            <div class="term-source">Tokenization</div>
        </div>
        
        <div class="term-row" data-search="bpe (byte pair encoding) algorithm: start with characters, merge most frequent pairs, repeat">
            <div class="term-name">BPE (Byte Pair Encoding)</div>
            <div class="term-def">Algorithm: start with characters, merge most frequent pairs, repeat</div>
            <div class="term-source">Tokenization</div>
        </div>
        
        <div class="term-row" data-search="sub-word tokenization bigger than characters, smaller than words (the sweet spot)">
            <div class="term-name">Sub-word tokenization</div>
            <div class="term-def">Bigger than characters, smaller than words (the sweet spot)</div>
            <div class="term-source">Tokenization</div>
        </div>
        
        <div class="term-row" data-search="token ids the numbers that represent each piece of text">
            <div class="term-name">Token IDs</div>
            <div class="term-def">The numbers that represent each piece of text</div>
            <div class="term-source">Tokenization</div>
        </div>
        
        <div class="term-row" data-search="attention mechanism lets each word "look at" every other word to understand context">
            <div class="term-name">Attention Mechanism</div>
            <div class="term-def">Lets each word "look at" every other word to understand context</div>
            <div class="term-source">Transformers</div>
        </div>
        
        <div class="term-row" data-search="self-attention words in the same sentence attending to each other">
            <div class="term-name">Self-Attention</div>
            <div class="term-def">Words in the same sentence attending to each other</div>
            <div class="term-source">Transformers</div>
        </div>
        
        <div class="term-row" data-search="encoder reads and understands input (used in bert)">
            <div class="term-name">Encoder</div>
            <div class="term-def">Reads and understands input (used in BERT)</div>
            <div class="term-source">Transformers</div>
        </div>
        
        <div class="term-row" data-search="decoder generates output one token at a time (used in gpt, claude, llama)">
            <div class="term-name">Decoder</div>
            <div class="term-def">Generates output one token at a time (used in GPT, Claude, Llama)</div>
            <div class="term-source">Transformers</div>
        </div>
        
        <div class="term-row" data-search=""attention is all you need" the 2017 paper that started it all (google brain)">
            <div class="term-name">"Attention Is All You Need"</div>
            <div class="term-def">The 2017 paper that started it all (Google Brain)</div>
            <div class="term-source">Transformers</div>
        </div>
        
        <div class="term-row" data-search="jay alammar wrote the famous "illustrated transformer" guide">
            <div class="term-name">Jay Alammar</div>
            <div class="term-def">Wrote the famous "Illustrated Transformer" guide</div>
            <div class="term-source">Transformers</div>
        </div>
        
        <div class="term-row" data-search="embedding layer turns token ids into vectors (rich number-lists the model can work with)">
            <div class="term-name">Embedding Layer</div>
            <div class="term-def">Turns token IDs into vectors (rich number-lists the model can work with)</div>
            <div class="term-source">Model Architecture</div>
        </div>
        
        <div class="term-row" data-search="transformer block a stack of layers (attention + linear + more) that process vectors">
            <div class="term-name">Transformer Block</div>
            <div class="term-def">A stack of layers (attention + linear + more) that process vectors</div>
            <div class="term-source">Model Architecture</div>
        </div>
        
        <div class="term-row" data-search="decoder-only transformer only the decoder half â€” used by all modern llms (gpt, claude, llama)">
            <div class="term-name">Decoder-only Transformer</div>
            <div class="term-def">Only the decoder half â€” used by ALL modern LLMs (GPT, Claude, Llama)</div>
            <div class="term-source">Model Architecture</div>
        </div>
        
        <div class="term-row" data-search="softmax the operation that turns numbers into probabilities (all sum to 1)">
            <div class="term-name">Softmax</div>
            <div class="term-def">The operation that turns numbers into probabilities (all sum to 1)</div>
            <div class="term-source">Model Architecture</div>
        </div>
        
        <div class="term-row" data-search="lm head (linear + softmax) final stage that picks the next token">
            <div class="term-name">LM Head (Linear + Softmax)</div>
            <div class="term-def">Final stage that picks the next token</div>
            <div class="term-source">Model Architecture</div>
        </div>
        
        <div class="term-row" data-search="hyperparameters settings that control model size (layers, dimension)">
            <div class="term-name">Hyperparameters</div>
            <div class="term-def">Settings that control model size (layers, dimension)</div>
            <div class="term-source">Model Architecture</div>
        </div>
        
        <div class="term-row" data-search="forward pass model reads text and guesses the next word">
            <div class="term-name">Forward Pass</div>
            <div class="term-def">Model reads text and guesses the next word</div>
            <div class="term-source">Model Training</div>
        </div>
        
        <div class="term-row" data-search="loss (cross-entropy) how wrong the guess was (3.0 = bad, 0.14 = good)">
            <div class="term-name">Loss (Cross-Entropy)</div>
            <div class="term-def">How wrong the guess was (3.0 = bad, 0.14 = good)</div>
            <div class="term-source">Model Training</div>
        </div>
        
        <div class="term-row" data-search="backward pass (backpropagation) optimizer calculates how to fix each weight">
            <div class="term-name">Backward Pass (Backpropagation)</div>
            <div class="term-def">Optimizer calculates how to fix each weight</div>
            <div class="term-source">Model Training</div>
        </div>
        
        <div class="term-row" data-search="optimizer (adam) the robot that adjusts billions of weights based on loss">
            <div class="term-name">Optimizer (Adam)</div>
            <div class="term-def">The robot that adjusts billions of weights based on loss</div>
            <div class="term-source">Model Training</div>
        </div>
        
        <div class="term-row" data-search="weights/parameters the billions of tiny knobs that determine what the model knows">
            <div class="term-name">Weights/Parameters</div>
            <div class="term-def">The billions of tiny knobs that determine what the model knows</div>
            <div class="term-source">Model Training</div>
        </div>
        
        <div class="term-row" data-search="implicit knowledge model discovers patterns from statistics, not explicit teaching">
            <div class="term-name">Implicit Knowledge</div>
            <div class="term-def">Model discovers patterns from statistics, not explicit teaching</div>
            <div class="term-source">Model Training</div>
        </div>
        
        <div class="term-row" data-search="autoregressive generation predicting one token at a time, feeding it back as input">
            <div class="term-name">Autoregressive Generation</div>
            <div class="term-def">Predicting one token at a time, feeding it back as input</div>
            <div class="term-source">Text Generation</div>
        </div>
        
        <div class="term-row" data-search="greedy search always pick highest probability token (simple but repetitive)">
            <div class="term-name">Greedy Search</div>
            <div class="term-def">Always pick highest probability token (simple but repetitive)</div>
            <div class="term-source">Text Generation</div>
        </div>
        
        <div class="term-row" data-search="beam search track top-k paths simultaneously (better but still deterministic)">
            <div class="term-name">Beam Search</div>
            <div class="term-def">Track top-K paths simultaneously (better but still deterministic)</div>
            <div class="term-source">Text Generation</div>
        </div>
        
        <div class="term-row" data-search="multinomial sampling sample randomly according to probabilities (too risky)">
            <div class="term-name">Multinomial Sampling</div>
            <div class="term-def">Sample randomly according to probabilities (too risky)</div>
            <div class="term-source">Text Generation</div>
        </div>
        
        <div class="term-row" data-search="top-k sampling keep top-k tokens, sample from those (fixed k is the problem)">
            <div class="term-name">Top-K Sampling</div>
            <div class="term-def">Keep top-K tokens, sample from those (fixed K is the problem)</div>
            <div class="term-source">Text Generation</div>
        </div>
        
        <div class="term-row" data-search="top-p sampling (nucleus) keep tokens until cumulative probability > p, then sample (dynamic k â€” used in practice)">
            <div class="term-name">Top-P Sampling (Nucleus)</div>
            <div class="term-def">Keep tokens until cumulative probability > P, then sample (dynamic K â€” USED IN PRACTICE)</div>
            <div class="term-source">Text Generation</div>
        </div>
        
        <div class="term-row" data-search="temperature hyperparameter that sharpens (low) or smooths (high) probabilities">
            <div class="term-name">Temperature</div>
            <div class="term-def">Hyperparameter that sharpens (low) or smooths (high) probabilities</div>
            <div class="term-source">Text Generation</div>
        </div>
        
        <div class="term-row" data-search="deterministic same input â†’ same output always (greedy, beam)">
            <div class="term-name">Deterministic</div>
            <div class="term-def">Same input â†’ same output always (Greedy, Beam)</div>
            <div class="term-source">Text Generation</div>
        </div>
        
        <div class="term-row" data-search="stochastic same input â†’ different outputs each time (top-k, top-p)">
            <div class="term-name">Stochastic</div>
            <div class="term-def">Same input â†’ different outputs each time (Top-K, Top-P)</div>
            <div class="term-source">Text Generation</div>
        </div>
        
        <div class="term-row" data-search="sft (supervised fine-tuning) teaching the base model to follow instructions using q&a pairs">
            <div class="term-name">SFT (Supervised Fine-Tuning)</div>
            <div class="term-def">Teaching the base model to follow instructions using Q&A pairs</div>
            <div class="term-source">Post Training (SFT)</div>
        </div>
        
        <div class="term-row" data-search="base model pre-trained model that knows language but can't chat properly">
            <div class="term-name">Base Model</div>
            <div class="term-def">Pre-trained model that knows language but can't chat properly</div>
            <div class="term-source">Post Training (SFT)</div>
        </div>
        
        <div class="term-row" data-search="instruction-response pairs "q: what is 2+2? a: 4" format training data">
            <div class="term-name">Instruction-Response Pairs</div>
            <div class="term-def">"Q: What is 2+2? A: 4" format training data</div>
            <div class="term-source">Post Training (SFT)</div>
        </div>
        
        <div class="term-row" data-search="rl (reinforcement learning) second stage: teaches model to be safe and honest">
            <div class="term-name">RL (Reinforcement Learning)</div>
            <div class="term-def">Second stage: teaches model to be safe and honest</div>
            <div class="term-source">Post Training (SFT)</div>
        </div>
        
        <div class="term-row" data-search="data quality > quantity 14k perfect examples beat millions of mediocre ones">
            <div class="term-name">Data Quality > Quantity</div>
            <div class="term-def">14K perfect examples beat millions of mediocre ones</div>
            <div class="term-source">Post Training (SFT)</div>
        </div>
        
        <div class="term-row" data-search="reinforcement learning (rl) practice algorithm: model generates multiple responses, gets scored, reinforces high-scoring ones">
            <div class="term-name">Reinforcement Learning (RL)</div>
            <div class="term-def">Practice algorithm: model generates multiple responses, gets scored, reinforces high-scoring ones</div>
            <div class="term-source">Reinforcement Learning (RL)</div>
        </div>
        
        <div class="term-row" data-search="verifiable tasks math, coding â€” can be auto-checked (python script verifies answer)">
            <div class="term-name">Verifiable Tasks</div>
            <div class="term-def">Math, coding â€” can be auto-checked (Python script verifies answer)</div>
            <div class="term-source">Reinforcement Learning (RL)</div>
        </div>
        
        <div class="term-row" data-search="unverifiable tasks writing, brainstorming â€” subjective, need human judgment">
            <div class="term-name">Unverifiable Tasks</div>
            <div class="term-def">Writing, brainstorming â€” subjective, need human judgment</div>
            <div class="term-source">Reinforcement Learning (RL)</div>
        </div>
        
        <div class="term-row" data-search="rlhf reinforcement learning from human feedback (for unverifiable tasks)">
            <div class="term-name">RLHF</div>
            <div class="term-def">Reinforcement Learning from Human Feedback (for unverifiable tasks)</div>
            <div class="term-source">Reinforcement Learning (RL)</div>
        </div>
        
        <div class="term-row" data-search="reward model separate model trained to score responses as a proxy for humans">
            <div class="term-name">Reward Model</div>
            <div class="term-def">Separate model trained to score responses as a proxy for humans</div>
            <div class="term-source">Reinforcement Learning (RL)</div>
        </div>
        
        <div class="term-row" data-search="margin ranking loss loss function that maximizes gap between winning and losing response scores">
            <div class="term-name">Margin Ranking Loss</div>
            <div class="term-def">Loss function that maximizes gap between winning and losing response scores</div>
            <div class="term-source">Reinforcement Learning (RL)</div>
        </div>
        
        <div class="term-row" data-search="ppo (proximal policy optimization) rl algorithm that updates model parameters to reinforce good responses">
            <div class="term-name">PPO (Proximal Policy Optimization)</div>
            <div class="term-def">RL algorithm that updates model parameters to reinforce good responses</div>
            <div class="term-source">Reinforcement Learning (RL)</div>
        </div>
        
        <div class="term-row" data-search="grpo newer alternative to ppo">
            <div class="term-name">GRPO</div>
            <div class="term-def">Newer alternative to PPO</div>
            <div class="term-source">Reinforcement Learning (RL)</div>
        </div>
        
        <div class="term-row" data-search="winning/losing pairs training data for reward model: human-ranked response pairs">
            <div class="term-name">Winning/Losing Pairs</div>
            <div class="term-def">Training data for reward model: human-ranked response pairs</div>
            <div class="term-source">Reinforcement Learning (RL)</div>
        </div>
        
        <div class="term-row" data-search="offline evaluation testing in lab environment before deployment">
            <div class="term-name">Offline Evaluation</div>
            <div class="term-def">Testing in lab environment BEFORE deployment</div>
            <div class="term-source">Evaluation of LLMs + System Design</div>
        </div>
        
        <div class="term-row" data-search="online evaluation testing in production after deployment with real users">
            <div class="term-name">Online Evaluation</div>
            <div class="term-def">Testing in production AFTER deployment with real users</div>
            <div class="term-source">Evaluation of LLMs + System Design</div>
        </div>
        
        <div class="term-row" data-search="perplexity traditional metric: how well model predicts next tokens (lower = better, but no longer meaningful)">
            <div class="term-name">Perplexity</div>
            <div class="term-def">Traditional metric: how well model predicts next tokens (lower = better, but no longer meaningful)</div>
            <div class="term-source">Evaluation of LLMs + System Design</div>
        </div>
        
        <div class="term-row" data-search="task-specific benchmarks standardized tests: piqa, hellaswag (common sense), gsm8k (math), humaneval (code), mmlu (composite)">
            <div class="term-name">Task-Specific Benchmarks</div>
            <div class="term-def">Standardized tests: PIQA, HellaSwag (common sense), GSM8K (math), HumanEval (code), MMLU (composite)</div>
            <div class="term-source">Evaluation of LLMs + System Design</div>
        </div>
        
        <div class="term-row" data-search="human evaluation hire experts to ask hard questions (pro: nuance, con: biased + subjective)">
            <div class="term-name">Human Evaluation</div>
            <div class="term-def">Hire experts to ask hard questions (pro: nuance, con: biased + subjective)</div>
            <div class="term-source">Evaluation of LLMs + System Design</div>
        </div>
        
        <div class="term-row" data-search="human feedback thumbs up/down in chatgpt ui (also feeds back into rl training)">
            <div class="term-name">Human Feedback</div>
            <div class="term-def">Thumbs up/down in ChatGPT UI (also feeds back into RL training)</div>
            <div class="term-source">Evaluation of LLMs + System Design</div>
        </div>
        
        <div class="term-row" data-search="lm arena (lmarena.ai) crowdsourcing platform by berkeley grads: blind comparison, 2 anonymous models, users vote, elo ranking, 2.9m+ votes">
            <div class="term-name">LM Arena (lmarena.ai)</div>
            <div class="term-def">Crowdsourcing platform by Berkeley grads: blind comparison, 2 anonymous models, users vote, ELO ranking, 2.9M+ votes</div>
            <div class="term-source">Evaluation of LLMs + System Design</div>
        </div>
        
        <div class="term-row" data-search="safety filtering input guardrails: checks if user's prompt is safe">
            <div class="term-name">Safety Filtering</div>
            <div class="term-def">Input guardrails: checks if user's prompt is safe</div>
            <div class="term-source">Evaluation of LLMs + System Design</div>
        </div>
        
        <div class="term-row" data-search="prompt enhancer fixes typos, grammar, ambiguity (ml + heuristics)">
            <div class="term-name">Prompt Enhancer</div>
            <div class="term-def">Fixes typos, grammar, ambiguity (ML + heuristics)</div>
            <div class="term-source">Evaluation of LLMs + System Design</div>
        </div>
        
        <div class="term-row" data-search="response safety evaluator output guardrails: checks if model's response is safe">
            <div class="term-name">Response Safety Evaluator</div>
            <div class="term-def">Output guardrails: checks if model's response is safe</div>
            <div class="term-source">Evaluation of LLMs + System Design</div>
        </div>
        
        <div class="term-row" data-search="session management appends full chat history to every new prompt (how chatgpt "remembers")">
            <div class="term-name">Session Management</div>
            <div class="term-def">Appends full chat history to every new prompt (how ChatGPT "remembers")</div>
            <div class="term-source">Evaluation of LLMs + System Design</div>
        </div>
        
            </div>

            <!-- 2-Min Script -->
            <div class="page" id="script">
                <div class="page-breadcrumb">Interview Prep</div>
                <h1 class="page-title">Explain It All in 2 Minutes</h1>
                <div class="page-meta">Read this out loud. If you can say it naturally, you own every concept.</div>

                <div class="script-box">
                    <p class="script-para">So you want to know how ChatGPT works? Here's the whole story:</p>
<p class="script-para">First, they vacuumed up the entire internet â€” 2.7 billion web pages â€” and cleaned it. Then they built a translator called a tokenizer that converts words into numbers, because computers can't read text.</p>
<p class="script-para">Those numbers go through 4 stages: the tokenizer chops text into IDs, an embedding layer turns IDs into rich vectors, transformer blocks process those vectors through dozens of smart filters, and a softmax function picks the most likely next word out of 50,000 options.</p>
<p class="script-para">To teach it, they run a loop a million times: guess the next word, check how wrong it was, adjust billions of tiny knobs, repeat. This costs $100 million and takes months on thousands of GPUs.</p>
<p class="script-para">But a trained model only gives you probabilities â€” it doesn't actually write text yet. Text generation is a for loop: pick one token using Top-P sampling (keep the most likely words until they add up to 90%, randomly pick from those), add it to the sentence, repeat. Temperature controls creativity â€” low for code, high for stories.</p>
<p class="script-para">After all that, the model is a genius who can't hold a conversation. So they show it 14,000 perfect Q&amp;A examples (that's SFT â€” supervised fine-tuning), and it learns to answer questions.</p>
<p class="script-para">But it's still not great â€” answers are correct but lazy and sometimes unsafe. So they run reinforcement learning: let the model practice by generating multiple responses, score them (automatically for math/code, or using a reward model trained on human rankings for everything else), and PPO reinforces the high-scoring ones. The result? Detailed, helpful, safe, polite responses.</p>
<p class="script-para">But how do you know if the model is any good? Evaluation comes in two flavors: offline (before deployment) and online (after). Offline uses benchmarks like PIQA for common sense and HumanEval for coding â€” standardized tests with right answers. Online uses real users: thumbs up/down in ChatGPT, or LM Arena where Berkeley grads built a blind taste test â€” two anonymous models respond, you vote, identities revealed, ELO rankings updated. 2.9 million votes across 211+ models.</p>
<p class="script-para">And the model itself? It's just one piece. The full system has safety filters checking both input AND output, a prompt enhancer fixing your typos and grammar, the response generator using Top-P sampling, and session management that appends your entire chat history to every new prompt â€” that's how it 'remembers' your conversation.</p>
<p class="script-para">And that's it â€” that's ChatGPT, Claude, Llama. Same recipe: vacuum internet â†’ clean â†’ tokenize â†’ train â†’ generate â†’ SFT â†’ RL â†’ evaluate â†’ wrap in safety + session management. Some just use bigger LEGO towers than others.</p>

                </div>
            </div>

        </div>
    </div>

    <div class="toast" id="toast"></div>

    <script>
        const storyShareData = [{"topic": "4-Step LLM Learning Path", "kid_story": "Imagine you want to build a house. Would you start by putting up the roof? No way! You'd start with the foundation, then walls, then the roof, and finally the furniture inside. \ud83c\udfd7\ufe0f\n\nLearning AI works EXACTLY the same way. There are 4 steps, and you MUST do them in order:\n\nStep 1 is learning the basics \u2014 what is a neural network? How does a machine learn from data? This is your foundation. Skip this, and everything else feels like magic you can't explain. \ud83e\uddf1\n\nStep 2 is learning about Transformers \u2014"}, {"topic": "How LLMs Are Built + Data Prep", "kid_story": "Imagine you want to teach a robot to speak every language in the world. Where do you start? The INTERNET! \ud83c\udf10\n\nFirst, you send out a web crawler \u2014 a little spider bot that visits 2.7 BILLION web pages and copies everything it finds. That's 200-400 terabytes of raw messy text. Think of it like vacuuming the entire internet into a giant bag. \ud83d\udd77\ufe0f\n\nBut that bag is FILTHY \u2014 it's got HTML code, duplicate pages, people's private info, and toxic content. So you run it through a cleaning machine. Common Cra"}, {"topic": "Tokenization", "kid_story": "Here's a problem: computers don't understand words. They ONLY understand numbers. So how do you get a computer to read \"Tell me a joke\"? You need a translator! That translator is called a Tokenizer. \ud83d\udd22\n\nThe tokenizer has a dictionary \u2014 a vocabulary \u2014 with every word-piece it knows and a number for each one. \"Tell\" = 41, \"me\" = 553, \"a\" = 264, \"joke\" = 22691. Now the computer can work with it!\n\nBut how big should the dictionary be? If you use WHOLE WORDS, your dictionary needs 500,000+ entries (ev"}, {"topic": "Transformers", "kid_story": "In 2017, a team at Google published a paper called \"Attention Is All You Need\" and changed EVERYTHING. Before this, AI read text one word at a time, like reading through a tiny keyhole \u2014 slow and forgetful. \ud83d\udd11\n\nThe Transformer reads ALL words at once! And it has a superpower called \"attention\" \u2014 it figures out which words are important to each other. Read this: \"The cat sat on the mat because IT was tired.\" What does \"it\" mean? The cat or the mat? YOU know it's the cat. The Transformer figures th"}, {"topic": "Model Architecture", "kid_story": "Imagine you're playing a word-guessing game with a really smart robot. You say \"I hope you are\" and the robot has to guess what comes next. \ud83e\udd16\n\nBut here's the thing \u2014 the robot can't read words! It only understands numbers. So first, a helper called the Tokenizer chops your words into secret number codes. \"I\" becomes 42, \"hope\" becomes 891, and so on.\n\nBut plain numbers aren't enough. The robot needs RICHER numbers \u2014 like a whole list of clues for each word. So the Embedding Layer turns each numb"}, {"topic": "Model Training", "kid_story": "Imagine you build a robot that finishes your sentences. You say \"peanut butter and...\" and it should say \"jelly!\" But when you first turn it on, it says \"elephant!\" because its brain is full of random nonsense. \ud83e\udd16\n\nSo here's how you teach it. You grab a sentence from a book \u2014 \"The cat sat on the...\" \u2014 and you KNOW the answer is \"mat.\" You let the robot guess. It says \"pizza!\" with 15% confidence. Then you use a GRADE MACHINE (that's the loss function) \u2014 it compares \"pizza\" with \"mat\" and gives a "}, {"topic": "Text Generation", "kid_story": "Imagine you have a super smart robot that knows every word in every book ever written. You ask it to finish your sentence: \"Once upon a...\" But the robot doesn't just KNOW the answer \u2014 it has a list of 50,000 possible next words, each with a score of how likely it is. \ud83e\udd16\n\nSo how does it pick? The lazy way (Greedy Search) is to always pick the word with the highest score. \"time\" wins at 40%! Easy! But here's the problem \u2014 the robot gets stuck in a loop. Like a kid who tells the same joke over and "}, {"topic": "Post Training SFT", "kid_story": "Remember that genius who never learned manners? The one who read the entire internet but can't hold a conversation? That's a base model after pre-training. It can finish any sentence, but ask it \"What's the capital of France?\" and it might just keep rambling instead of answering. \ud83e\udd37\n\nSFT \u2014 Supervised Fine-Tuning \u2014 is like sending that genius to finishing school. You give it 14,000 to 100,000 examples of perfect conversations: \"Question: What's 2+2? Answer: 4.\" Over and over, until it learns the F"}, {"topic": "Reinforcement Learning", "kid_story": "Imagine you taught a robot to answer homework questions (that was SFT). It answers, but it's lazy \u2014 ask \"how do I learn math?\" and it says \"read a book.\" Technically correct, but not helpful! \ud83d\ude24\n\nSo you give the robot a practice drill. You ask it the same question 7 times and it gives 7 different answers. Some are great (\"start with addition, then multiplication, then practice with fun games!\") and some are terrible (\"just give up\" \ud83d\ude05). You mark the good ones with \u2705 and bad ones with \u274c, then tell "}, {"topic": "Evaluation of LLMs", "kid_story": "Imagine your school is having a bake-off and you need to judge whose cookies are best! \ud83c\udf6a\n\nFirst, you try judging in your kitchen BEFORE the bake sale \u2014 that's offline evaluation. You have three ways: You could count ingredients exactly (perplexity \u2014 \"did they follow the recipe perfectly?\") but that's boring because perfect-recipe cookies can still taste terrible! \ud83d\ude1d So instead you give them REAL tests \u2014 \"make chocolate chip cookies, make brownies, make a cake\" \u2014 and see how each one turns out. Th"}];

        function shareStory(index) {
            const story = storyShareData[index];
            if (!story) return;
            const subject = encodeURIComponent('Check out this AI story: ' + story.topic);
            const dashboardUrl = 'https://kneil31.github.io/ai-learning/';
            const preview = story.kid_story.substring(0, 300) + (story.kid_story.length > 300 ? '...' : '');
            const body = encodeURIComponent(
                'ğŸ“– ' + story.topic + '\n\n' +
                preview + '\n\n' +
                'ğŸ”— Read the full story (with diagrams): ' + dashboardUrl + '\n\n' +
                'â€” From Ram\'s AI Learning Journey'
            );
            const mailtoLink = 'mailto:?subject=' + subject + '&body=' + body;
            const a = document.createElement('a');
            a.href = mailtoLink;
            a.target = '_blank';
            a.rel = 'noreferrer noopener';
            a.click();
        }

        function showSection(id) {
            // Hide all pages
            document.querySelectorAll('.page').forEach(p => p.classList.remove('active'));
            // Show target
            const target = document.getElementById(id);
            if (target) target.classList.add('active');
            // Update sidebar active state
            document.querySelectorAll('.sidebar-link').forEach(a => a.classList.remove('active'));
            if (event && event.currentTarget) event.currentTarget.classList.add('active');
            // Also highlight matching sidebar link by href
            document.querySelectorAll('.sidebar-link').forEach(a => {
                if (a.getAttribute('href') === '#' + id) a.classList.add('active');
            });
            // Scroll content to top
            document.querySelector('.content').scrollTop = 0;
        }

        function navigateTo(sectionId) {
            // Used by pipeline steps to jump to a story page
            document.querySelectorAll('.page').forEach(p => p.classList.remove('active'));
            const target = document.getElementById(sectionId);
            if (target) target.classList.add('active');
            // Update sidebar
            document.querySelectorAll('.sidebar-link').forEach(a => {
                a.classList.remove('active');
                if (a.getAttribute('href') === '#' + sectionId) a.classList.add('active');
            });
            document.querySelector('.content').scrollTop = 0;
        }

        function showToast(msg) {
            const t = document.getElementById('toast');
            t.textContent = msg;
            t.classList.add('show');
            setTimeout(() => t.classList.remove('show'), 2500);
        }

        function shareDashboard(e) {
            e.preventDefault();
            // Try native share first (works on mobile + some desktops)
            if (navigator.share) {
                const blob = new Blob([document.documentElement.outerHTML], {type: 'text/html'});
                const file = new File([blob], 'ai_learning_dashboard.html', {type: 'text/html'});
                navigator.share({
                    title: 'AI Learning Dashboard',
                    text: 'Check out my AI learning journey â€” stories, pipeline, and key terms!',
                    files: [file]
                }).catch(() => fallbackShare());
            } else {
                fallbackShare();
            }
        }

        function fallbackShare() {
            // Download the HTML file so they can share it
            const blob = new Blob([document.documentElement.outerHTML], {type: 'text/html'});
            const url = URL.createObjectURL(blob);
            const a = document.createElement('a');
            a.href = url;
            a.download = 'ai_learning_dashboard.html';
            a.click();
            URL.revokeObjectURL(url);
            showToast('ğŸ“¥ Downloaded! Send this file to anyone â€” it works standalone.');
        }

        function filterTerms(query) {
            const rows = document.querySelectorAll('.term-row');
            const q = query.toLowerCase();
            rows.forEach(row => {
                const text = row.getAttribute('data-search');
                row.classList.toggle('hidden', q !== '' && !text.includes(q));
            });
        }
    </script>
</body>
</html>
